{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OrangeSum FR POINTER generate training data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "152efnr7AmFe",
        "OLUMUutIAtuE",
        "d3pNqXI3BC7w",
        "RCaS3OT-BI2I",
        "TqN10GRBBOJF",
        "cVM8giu2BaDY",
        "a9xZ1C0yBsvH",
        "zb8gB3iAB0cw",
        "cviRGS5vTtpq",
        "Q1aYGeghT2l5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "152efnr7AmFe"
      },
      "source": [
        "### Installations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usVEFA9LF0br"
      },
      "source": [
        "%pip install yake boto3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cNVBbZxDzen"
      },
      "source": [
        "%pip install codecarbon comet_ml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzw0GGe0QjfT"
      },
      "source": [
        "%pip install spacy\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCaS3OT-BI2I"
      },
      "source": [
        "### Begin Comet experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e97sCJzyD-vM"
      },
      "source": [
        "from comet_ml import Experiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqN10GRBBOJF"
      },
      "source": [
        "### Define utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDP2cp2CpyaE"
      },
      "source": [
        "# Cached_path definition\n",
        "\n",
        "\"\"\"\n",
        "Utilities for working with the local dataset cache.\n",
        "This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\n",
        "Copyright by the AllenNLP authors.\n",
        "\"\"\"\n",
        "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import fnmatch\n",
        "import spacy\n",
        "from functools import wraps\n",
        "from hashlib import sha256\n",
        "from io import open\n",
        "\n",
        "import boto3\n",
        "import requests\n",
        "from botocore.exceptions import ClientError\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from torch.hub import _get_torch_home\n",
        "    torch_cache_home = _get_torch_home()\n",
        "except ImportError:\n",
        "    torch_cache_home = os.path.expanduser(\n",
        "        os.getenv('TORCH_HOME', os.path.join(\n",
        "            os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n",
        "default_cache_path = os.path.join(torch_cache_home, 'pytorch_transformers')\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "\n",
        "try:\n",
        "    from pathlib import Path\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
        "        os.getenv('PYTORCH_TRANSFORMERS_CACHE', os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path)))\n",
        "except (AttributeError, ImportError):\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_TRANSFORMERS_CACHE',\n",
        "                                              os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                                        default_cache_path))\n",
        "\n",
        "PYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "def url_to_filename(url, etag=None):\n",
        "    \"\"\"\n",
        "    Convert `url` into a hashed filename in a repeatable way.\n",
        "    If `etag` is specified, append its hash to the url's, delimited\n",
        "    by a period.\n",
        "    \"\"\"\n",
        "    url_bytes = url.encode('utf-8')\n",
        "    url_hash = sha256(url_bytes)\n",
        "    filename = url_hash.hexdigest()\n",
        "\n",
        "    if etag:\n",
        "        etag_bytes = etag.encode('utf-8')\n",
        "        etag_hash = sha256(etag_bytes)\n",
        "        filename += '.' + etag_hash.hexdigest()\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def filename_to_url(filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
        "    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "    if not os.path.exists(cache_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
        "\n",
        "    meta_path = cache_path + '.json'\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
        "\n",
        "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
        "        metadata = json.load(meta_file)\n",
        "    url = metadata['url']\n",
        "    etag = metadata['etag']\n",
        "\n",
        "    return url, etag\n",
        "\n",
        "\n",
        "def cached_path(url_or_filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given something that might be a URL (or might be a local path),\n",
        "    determine which. If it's a URL, download the file and cache it, and\n",
        "    return the path to the cached file. If it's already a local path,\n",
        "    make sure the file exists and then return the path.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n",
        "        url_or_filename = str(url_or_filename)\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    parsed = urlparse(url_or_filename)\n",
        "\n",
        "    if parsed.scheme in ('http', 'https', 's3'):\n",
        "        # URL, so get it from the cache (downloading if necessary)\n",
        "        return get_from_cache(url_or_filename, cache_dir)\n",
        "    elif os.path.exists(url_or_filename):\n",
        "        # File, and it exists.\n",
        "        return url_or_filename\n",
        "    elif parsed.scheme == '':\n",
        "        # File, but it doesn't exist.\n",
        "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
        "    else:\n",
        "        # Something unknown\n",
        "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
        "\n",
        "\n",
        "def split_s3_path(url):\n",
        "    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.netloc or not parsed.path:\n",
        "        raise ValueError(\"bad s3 path {}\".format(url))\n",
        "    bucket_name = parsed.netloc\n",
        "    s3_path = parsed.path\n",
        "    # Remove '/' at beginning of path.\n",
        "    if s3_path.startswith(\"/\"):\n",
        "        s3_path = s3_path[1:]\n",
        "    return bucket_name, s3_path\n",
        "\n",
        "\n",
        "def s3_request(func):\n",
        "    \"\"\"\n",
        "    Wrapper function for s3 requests in order to create more helpful error\n",
        "    messages.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(url, *args, **kwargs):\n",
        "        try:\n",
        "            return func(url, *args, **kwargs)\n",
        "        except ClientError as exc:\n",
        "            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n",
        "                raise EnvironmentError(\"file {} not found\".format(url))\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_etag(url):\n",
        "    \"\"\"Check ETag on S3 object.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_object = s3_resource.Object(bucket_name, s3_path)\n",
        "    return s3_object.e_tag\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_get(url, temp_file):\n",
        "    \"\"\"Pull a file directly from S3.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n",
        "\n",
        "\n",
        "def http_get(url, temp_file):\n",
        "    req = requests.get(url, stream=True)\n",
        "    content_length = req.headers.get('Content-Length')\n",
        "    total = int(content_length) if content_length is not None else None\n",
        "    progress = tqdm(unit=\"B\", total=total)\n",
        "    for chunk in req.iter_content(chunk_size=1024):\n",
        "        if chunk: # filter out keep-alive new chunks\n",
        "            progress.update(len(chunk))\n",
        "            temp_file.write(chunk)\n",
        "    progress.close()\n",
        "\n",
        "\n",
        "def get_from_cache(url, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given a URL, look for the corresponding dataset in the local cache.\n",
        "    If it's not there, download it. Then return the path to the cached file.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "    if sys.version_info[0] == 2 and not isinstance(cache_dir, str):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    # Get eTag to add to filename, if it exists.\n",
        "    if url.startswith(\"s3://\"):\n",
        "        etag = s3_etag(url)\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True)\n",
        "            if response.status_code != 200:\n",
        "                etag = None\n",
        "            else:\n",
        "                etag = response.headers.get(\"ETag\")\n",
        "        except EnvironmentError:\n",
        "            etag = None\n",
        "\n",
        "    if sys.version_info[0] == 2 and etag is not None:\n",
        "        etag = etag.decode('utf-8')\n",
        "    filename = url_to_filename(url, etag)\n",
        "\n",
        "    # get cache path to put the file\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "\n",
        "    # If we don't have a connection (etag is None) and can't identify the file\n",
        "    # try to get the last downloaded one\n",
        "    if not os.path.exists(cache_path) and etag is None:\n",
        "        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + '.*')\n",
        "        matching_files = list(filter(lambda s: not s.endswith('.json'), matching_files))\n",
        "        if matching_files:\n",
        "            cache_path = os.path.join(cache_dir, matching_files[-1])\n",
        "\n",
        "    if not os.path.exists(cache_path):\n",
        "        # Download to temporary file, then copy to cache dir once finished.\n",
        "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
        "        with tempfile.NamedTemporaryFile() as temp_file:\n",
        "            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n",
        "\n",
        "            # GET file object\n",
        "            if url.startswith(\"s3://\"):\n",
        "                s3_get(url, temp_file)\n",
        "            else:\n",
        "                http_get(url, temp_file)\n",
        "\n",
        "            # we are copying the file before closing it, so flush to avoid truncation\n",
        "            temp_file.flush()\n",
        "            # shutil.copyfileobj() starts at the current position, so go to the start\n",
        "            temp_file.seek(0)\n",
        "\n",
        "            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n",
        "            with open(cache_path, 'wb') as cache_file:\n",
        "                shutil.copyfileobj(temp_file, cache_file)\n",
        "\n",
        "            logger.info(\"creating metadata file for %s\", cache_path)\n",
        "            meta = {'url': url, 'etag': etag}\n",
        "            meta_path = cache_path + '.json'\n",
        "            with open(meta_path, 'w') as meta_file:\n",
        "                output_string = json.dumps(meta)\n",
        "                if sys.version_info[0] == 2 and isinstance(output_string, str):\n",
        "                    output_string = unicode(output_string, 'utf-8')  # The beauty of python 2\n",
        "                meta_file.write(output_string)\n",
        "\n",
        "            logger.info(\"removing temp file %s\", temp_file.name)\n",
        "\n",
        "    return cache_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkAx8IvZpnho"
      },
      "source": [
        "#from .tokenization_utils import PreTrainedTokenizer\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n",
        "from __future__ import (absolute_import, division, print_function,\n",
        "                        unicode_literals)\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import six\n",
        "from io import open\n",
        "\n",
        "#from .file_utils import cached_path\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "SPECIAL_TOKENS_MAP_FILE = 'special_tokens_map.json'\n",
        "ADDED_TOKENS_FILE = 'added_tokens.json'\n",
        "\n",
        "class PreTrainedTokenizer(object):\n",
        "    \"\"\" Base class for all tokenizers.\n",
        "    Handle all the shared methods for tokenization and special tokens as well as methods dowloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\n",
        "    This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
        "    Class attributes (overridden by derived classes):\n",
        "        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).\n",
        "        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.\n",
        "        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.\n",
        "    Parameters:\n",
        "        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token``\n",
        "        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token``\n",
        "        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token``\n",
        "        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token``\n",
        "        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token``\n",
        "        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token``\n",
        "        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token``\n",
        "        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won't be split by the tokenization process. Will be associated to ``self.additional_special_tokens``\n",
        "    \"\"\"\n",
        "    vocab_files_names = {}\n",
        "    pretrained_vocab_files_map = {}\n",
        "    max_model_input_sizes = {}\n",
        "\n",
        "    SPECIAL_TOKENS_ATTRIBUTES = [\"bos_token\", \"eos_token\", \"unk_token\", \"sep_token\",\n",
        "                                 \"pad_token\", \"cls_token\", \"mask_token\", \"noi_token\",\n",
        "                                 \"additional_special_tokens\"]\n",
        "\n",
        "    @property\n",
        "    def bos_token(self):\n",
        "        \"\"\" Beginning of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._bos_token is None:\n",
        "            logger.error(\"Using bos_token, but it is not set yet.\")\n",
        "        return self._bos_token\n",
        "\n",
        "    @property\n",
        "    def eos_token(self):\n",
        "        \"\"\" End of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._eos_token is None:\n",
        "            logger.error(\"Using eos_token, but it is not set yet.\")\n",
        "        return self._eos_token\n",
        "\n",
        "    @property\n",
        "    def unk_token(self):\n",
        "        \"\"\" Unknown token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._unk_token is None:\n",
        "            logger.error(\"Using unk_token, but it is not set yet.\")\n",
        "        return self._unk_token\n",
        "\n",
        "    @property\n",
        "    def sep_token(self):\n",
        "        \"\"\" Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\n",
        "        if self._sep_token is None:\n",
        "            logger.error(\"Using sep_token, but it is not set yet.\")\n",
        "        return self._sep_token\n",
        "\n",
        "    @property\n",
        "    def pad_token(self):\n",
        "        \"\"\" Padding token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._pad_token is None:\n",
        "            logger.error(\"Using pad_token, but it is not set yet.\")\n",
        "        return self._pad_token\n",
        "\n",
        "    @property\n",
        "    def cls_token(self):\n",
        "        \"\"\" Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"\n",
        "        if self._cls_token is None:\n",
        "            logger.error(\"Using cls_token, but it is not set yet.\")\n",
        "        return self._cls_token\n",
        "\n",
        "    @property\n",
        "    def mask_token(self):\n",
        "        \"\"\" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
        "        if self._mask_token is None:\n",
        "            logger.error(\"Using mask_token, but it is not set yet.\")\n",
        "        return self._mask_token\n",
        "\n",
        "    @property\n",
        "    def noi_token(self):\n",
        "        \"\"\" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
        "        if self._noi_token is None:\n",
        "            logger.error(\"Using noi_token, but it is not set yet.\")\n",
        "        return self._noi_token\n",
        "\n",
        "    @property\n",
        "    def additional_special_tokens(self):\n",
        "        \"\"\" All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. \"\"\"\n",
        "        if self._additional_special_tokens is None:\n",
        "            logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n",
        "        return self._additional_special_tokens\n",
        "\n",
        "    @bos_token.setter\n",
        "    def bos_token(self, value):\n",
        "        self._bos_token = value\n",
        "\n",
        "    @eos_token.setter\n",
        "    def eos_token(self, value):\n",
        "        self._eos_token = value\n",
        "\n",
        "    @unk_token.setter\n",
        "    def unk_token(self, value):\n",
        "        self._unk_token = value\n",
        "\n",
        "    @sep_token.setter\n",
        "    def sep_token(self, value):\n",
        "        self._sep_token = value\n",
        "\n",
        "    @pad_token.setter\n",
        "    def pad_token(self, value):\n",
        "        self._pad_token = value\n",
        "\n",
        "    @cls_token.setter\n",
        "    def cls_token(self, value):\n",
        "        self._cls_token = value\n",
        "\n",
        "    @mask_token.setter\n",
        "    def mask_token(self, value):\n",
        "        self._mask_token = value\n",
        "    \n",
        "    @noi_token.setter\n",
        "    def noi_token(self, value):\n",
        "        self._noi_token = value\n",
        "\n",
        "    @additional_special_tokens.setter\n",
        "    def additional_special_tokens(self, value):\n",
        "        self._additional_special_tokens = value\n",
        "\n",
        "    def __init__(self, max_len=None, **kwargs):\n",
        "        self._bos_token = None\n",
        "        self._eos_token = None\n",
        "        self._unk_token = None\n",
        "        self._sep_token = None\n",
        "        self._pad_token = None\n",
        "        self._cls_token = None\n",
        "        self._mask_token = None\n",
        "        self._noi_token = None\n",
        "        self._additional_special_tokens = []\n",
        "\n",
        "        self.max_len = max_len if max_len is not None else int(1e12)\n",
        "        self.added_tokens_encoder = {}\n",
        "        self.added_tokens_decoder = {}\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
        "                if key == 'additional_special_tokens':\n",
        "                    assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n",
        "                else:\n",
        "                    assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n",
        "                setattr(self, key, value)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *inputs, **kwargs):\n",
        "        r\"\"\" Instantiate a :class:`~pytorch_transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
        "        Parameters:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~pytorch_transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
        "                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
        "            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
        "            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~pytorch_transformers.PreTrainedTokenizer` for details.\n",
        "        Examples::\n",
        "            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
        "            # Download vocabulary from S3 and cache.\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
        "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
        "            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
        "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
        "            # You can link tokens to special vocabulary when instantiating\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
        "            # You should be sure '<unk>' is in the vocabulary when doing that.\n",
        "            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
        "            assert tokenizer.unk_token == '<unk>'\n",
        "        \"\"\"\n",
        "        return cls._from_pretrained(*inputs, **kwargs)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def _from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\n",
        "\n",
        "        s3_models = list(cls.max_model_input_sizes.keys())\n",
        "        vocab_files = {}\n",
        "        if pretrained_model_name_or_path in s3_models:\n",
        "            # Get the vocabulary from AWS S3 bucket\n",
        "            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n",
        "                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            # Get the vocabulary from local files\n",
        "            logger.info(\n",
        "                \"Model name '{}' not found in model shortcut name list ({}). \"\n",
        "                \"Assuming '{}' is a path or url to a directory containing tokenizer files.\".format(\n",
        "                    pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                    pretrained_model_name_or_path))\n",
        "\n",
        "            # Look for the tokenizer main vocabulary files\n",
        "            for file_id, file_name in cls.vocab_files_names.items():\n",
        "                if os.path.isdir(pretrained_model_name_or_path):\n",
        "                    # If a directory is provided we look for the standard filenames\n",
        "                    full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n",
        "                else:\n",
        "                    # If a path to a file is provided we use it (will only work for non-BPE tokenizer using a single vocabulary file)\n",
        "                    full_file_name = pretrained_model_name_or_path\n",
        "                if not os.path.exists(full_file_name):\n",
        "                    logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
        "                    full_file_name = None\n",
        "                vocab_files[file_id] = full_file_name\n",
        "\n",
        "            # Look for the additional tokens files\n",
        "            all_vocab_files_names = {'added_tokens_file': ADDED_TOKENS_FILE,\n",
        "                                     'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE}\n",
        "\n",
        "            # If a path to a file was provided, get the parent directory\n",
        "            saved_directory = pretrained_model_name_or_path\n",
        "            if os.path.exists(saved_directory) and not os.path.isdir(saved_directory):\n",
        "                saved_directory = os.path.dirname(saved_directory)\n",
        "\n",
        "            for file_id, file_name in all_vocab_files_names.items():\n",
        "                full_file_name = os.path.join(saved_directory, file_name)\n",
        "                if not os.path.exists(full_file_name):\n",
        "                    logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
        "                    full_file_name = None\n",
        "                vocab_files[file_id] = full_file_name\n",
        "\n",
        "            if all(full_file_name is None for full_file_name in vocab_files.values()):\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find tokenizer files\"\n",
        "                    \"at this path or url.\".format(\n",
        "                        pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                        pretrained_model_name_or_path, ))\n",
        "                return None\n",
        "\n",
        "        # Get files from url, cache, or disk depending on the case\n",
        "        try:\n",
        "            resolved_vocab_files = {}\n",
        "            for file_id, file_path in vocab_files.items():\n",
        "                if file_path is None:\n",
        "                    resolved_vocab_files[file_id] = None\n",
        "                else:\n",
        "                    resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in s3_models:\n",
        "                logger.error(\"Couldn't reach server to download vocabulary.\")\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find files {} \"\n",
        "                    \"at this path or url.\".format(\n",
        "                        pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                        pretrained_model_name_or_path, str(vocab_files.keys())))\n",
        "            return None\n",
        "\n",
        "        for file_id, file_path in vocab_files.items():\n",
        "            if file_path == resolved_vocab_files[file_id]:\n",
        "                logger.info(\"loading file {}\".format(file_path))\n",
        "            else:\n",
        "                logger.info(\"loading file {} from cache at {}\".format(\n",
        "                    file_path, resolved_vocab_files[file_id]))\n",
        "\n",
        "        # Set max length if needed\n",
        "        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n",
        "            # if we're using a pretrained model, ensure the tokenizer\n",
        "            # wont index sequences longer than the number of positional embeddings\n",
        "            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n",
        "            if max_len is not None and isinstance(max_len, (int, float)):\n",
        "                kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
        "\n",
        "        # Merge resolved_vocab_files arguments in kwargs.\n",
        "        added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)\n",
        "        special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)\n",
        "        for args_name, file_path in resolved_vocab_files.items():\n",
        "            if args_name not in kwargs:\n",
        "                kwargs[args_name] = file_path\n",
        "        if special_tokens_map_file is not None:\n",
        "            special_tokens_map = json.load(open(special_tokens_map_file, encoding=\"utf-8\"))\n",
        "            for key, value in special_tokens_map.items():\n",
        "                if key not in kwargs:\n",
        "                    kwargs[key] = value\n",
        "\n",
        "        # Instantiate tokenizer.\n",
        "        tokenizer = cls(*inputs, **kwargs)\n",
        "\n",
        "        # Add supplementary tokens.\n",
        "        if added_tokens_file is not None:\n",
        "            added_tok_encoder = json.load(open(added_tokens_file, encoding=\"utf-8\"))\n",
        "            added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n",
        "            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n",
        "            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save the tokenizer vocabulary files (with added tokens) and the\n",
        "            special-tokens-to-class-attributes-mapping to a directory.\n",
        "            This method make sure the full tokenizer can then be re-loaded using the :func:`~pytorch_transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Saving directory ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n",
        "        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n",
        "\n",
        "        with open(special_tokens_map_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n",
        "\n",
        "        with open(added_tokens_file, 'w', encoding='utf-8') as f:\n",
        "            if self.added_tokens_encoder:\n",
        "                out_str = json.dumps(self.added_tokens_encoder, ensure_ascii=False)\n",
        "            else:\n",
        "                out_str = u\"{}\"\n",
        "            f.write(out_str)\n",
        "\n",
        "        vocab_files = self.save_vocabulary(save_directory)\n",
        "\n",
        "        return vocab_files + (special_tokens_map_file, added_tokens_file)\n",
        "\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n",
        "            and special token mappings.\n",
        "            Please use :func:`~pytorch_transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~pytorch_transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def vocab_size(self):\n",
        "        \"\"\" Size of the base vocabulary (without the added tokens) \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Size of the full vocabulary with the added tokens \"\"\"\n",
        "        return self.vocab_size + len(self.added_tokens_encoder)\n",
        "\n",
        "\n",
        "    def add_tokens(self, new_tokens):\n",
        "        \"\"\" Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n",
        "        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n",
        "            Parameters:\n",
        "                new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
        "            Returns:\n",
        "                Number of tokens added to the vocabulary.\n",
        "        Examples::\n",
        "            # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            model = BertModel.from_pretrained('bert-base-uncased')\n",
        "            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
        "            print('We have added', num_added_toks, 'tokens')\n",
        "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
        "        \"\"\"\n",
        "        if not new_tokens:\n",
        "            return 0\n",
        "\n",
        "        to_add_tokens = []\n",
        "        for token in new_tokens:\n",
        "            assert isinstance(token, str) or (six.PY2 and isinstance(token, unicode))\n",
        "            if token != self.unk_token and \\\n",
        "                    self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token):\n",
        "                to_add_tokens.append(token)\n",
        "                logger.info(\"Adding %s to the vocabulary\", token)\n",
        "\n",
        "        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n",
        "        added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n",
        "        self.added_tokens_encoder.update(added_tok_encoder)\n",
        "        self.added_tokens_decoder.update(added_tok_decoder)\n",
        "\n",
        "        return len(to_add_tokens)\n",
        "\n",
        "\n",
        "    def add_special_tokens(self, special_tokens_dict):\n",
        "        \"\"\" Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n",
        "            to class attributes. If special tokens are NOT in the vocabulary, they are added\n",
        "            to it (indexed starting from the last index of the current vocabulary).\n",
        "            Parameters:\n",
        "                special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``].\n",
        "                \n",
        "                    Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
        "            Returns:\n",
        "                Number of tokens added to the vocabulary.\n",
        "        Examples::\n",
        "            # Let's see how to add a new classification token to GPT-2\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            model = GPT2Model.from_pretrained('gpt2')\n",
        "            special_tokens_dict = {'cls_token': '<CLS>'}\n",
        "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            print('We have added', num_added_toks, 'tokens')\n",
        "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
        "            assert tokenizer.cls_token == '<CLS>'\n",
        "        \"\"\"\n",
        "        if not special_tokens_dict:\n",
        "            return 0\n",
        "\n",
        "        added_tokens = 0\n",
        "        for key, value in special_tokens_dict.items():\n",
        "            assert key in self.SPECIAL_TOKENS_ATTRIBUTES\n",
        "            if key == 'additional_special_tokens':\n",
        "                assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n",
        "                added_tokens += self.add_tokens(value)\n",
        "            else:\n",
        "                assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n",
        "                added_tokens += self.add_tokens([value])\n",
        "            logger.info(\"Assigning %s to the %s key of the tokenizer\", value, key)\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        return added_tokens\n",
        "\n",
        "    def tokenize(self, text, **kwargs):\n",
        "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
        "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
        "            vocabularies (BPE/SentencePieces/WordPieces).\n",
        "            Take care of added tokens.\n",
        "        \"\"\"\n",
        "        def split_on_tokens(tok_list, text):\n",
        "            if not text:\n",
        "                return []\n",
        "            if not tok_list:\n",
        "                return self._tokenize(text, **kwargs)\n",
        "            tok = tok_list[0]\n",
        "            split_text = text.split(tok)\n",
        "            return sum((split_on_tokens(tok_list[1:], sub_text.strip()) + [tok] \\\n",
        "                        for sub_text in split_text), [])[:-1]\n",
        "\n",
        "        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n",
        "        tokenized_text = split_on_tokens(added_tokens, text)\n",
        "        return tokenized_text\n",
        "\n",
        "    def _tokenize(self, text, **kwargs):\n",
        "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
        "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
        "            vocabularies (BPE/SentencePieces/WordPieces).\n",
        "            Do NOT take care of added tokens.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\" Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n",
        "            (resp. a sequence of ids), using the vocabulary.\n",
        "        \"\"\"\n",
        "        if isinstance(tokens, str) or (six.PY2 and isinstance(tokens, unicode)):\n",
        "            return self._convert_token_to_id_with_added_voc(tokens)\n",
        "\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            ids.append(self._convert_token_to_id_with_added_voc(token))\n",
        "        if len(ids) > self.max_len:\n",
        "            logger.warning(\"Token indices sequence length is longer than the specified maximum sequence length \"\n",
        "                           \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
        "                           \"indexing errors\".format(len(ids), self.max_len))\n",
        "        return ids\n",
        "\n",
        "    def _convert_token_to_id_with_added_voc(self, token):\n",
        "        if token in self.added_tokens_encoder:\n",
        "            return self.added_tokens_encoder[token]\n",
        "        return self._convert_token_to_id(token)\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\" Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
        "        \n",
        "        Same doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
        "        \"\"\"\n",
        "        return self.convert_tokens_to_ids(self.tokenize(text))\n",
        "\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n",
        "        \"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n",
        "            (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n",
        "            Args:\n",
        "                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n",
        "        \"\"\"\n",
        "        if isinstance(ids, int):\n",
        "            if ids in self.added_tokens_decoder:\n",
        "                return self.added_tokens_decoder[ids]\n",
        "            else:\n",
        "                return self._convert_id_to_token(ids)\n",
        "        tokens = []\n",
        "        for index in ids:\n",
        "            if index in self.all_special_ids and skip_special_tokens:\n",
        "                continue\n",
        "            if index in self.added_tokens_decoder:\n",
        "                tokens.append(self.added_tokens_decoder[index])\n",
        "            else:\n",
        "                tokens.append(self._convert_id_to_token(index))\n",
        "        return tokens\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string.\n",
        "            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n",
        "            but we often want to remove sub-word tokenization artifacts at the same time.\n",
        "        \"\"\"\n",
        "        return ' '.join(self.convert_ids_to_tokens(tokens))\n",
        "\n",
        "    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n",
        "        \"\"\" Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
        "            with options to remove special tokens and clean up tokenization spaces.\n",
        "        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
        "        \"\"\"\n",
        "        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n",
        "        text = self.convert_tokens_to_string(filtered_tokens)\n",
        "        if clean_up_tokenization_spaces:\n",
        "            text = self.clean_up_tokenization(text)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def special_tokens_map(self):\n",
        "        \"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n",
        "            values ('<unk>', '<cls>'...)\n",
        "        \"\"\"\n",
        "        set_attr = {}\n",
        "        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
        "            attr_value = getattr(self, \"_\" + attr)\n",
        "            if attr_value:\n",
        "                set_attr[attr] = attr_value\n",
        "        return set_attr\n",
        "\n",
        "    @property\n",
        "    def all_special_tokens(self):\n",
        "        \"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n",
        "            (cls_token, unk_token...).\n",
        "        \"\"\"\n",
        "        all_toks = []\n",
        "        set_attr = self.special_tokens_map\n",
        "        for attr_value in set_attr.values():\n",
        "            all_toks = all_toks + (attr_value if isinstance(attr_value, (list, tuple)) else [attr_value])\n",
        "        all_toks = list(set(all_toks))\n",
        "        return all_toks\n",
        "\n",
        "    @property\n",
        "    def all_special_ids(self):\n",
        "        \"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n",
        "            class attributes (cls_token, unk_token...).\n",
        "        \"\"\"\n",
        "        all_toks = self.all_special_tokens\n",
        "        all_ids = list(self.convert_tokens_to_ids(t) for t in all_toks)\n",
        "        return all_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_up_tokenization(out_string):\n",
        "        \"\"\" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
        "        \"\"\"\n",
        "        out_string = out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ','\n",
        "                        ).replace(\" ' \", \"'\").replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" do not\", \" don't\"\n",
        "                        ).replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 're\", \"'re\")\n",
        "        return out_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfEkhNMVo-SD"
      },
      "source": [
        "# from pytorch_transformers.tokenization_bert import BertTokenizer\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from io import open\n",
        "\n",
        "#from .tokenization_utils import PreTrainedTokenizer\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {'vocab_file': 'vocab.txt'}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    'vocab_file':\n",
        "    {\n",
        "        'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
        "        'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
        "        'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
        "        'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
        "        'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
        "        'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
        "        'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
        "        'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    'bert-base-uncased': 512,\n",
        "    'bert-large-uncased': 512,\n",
        "    'bert-base-cased': 512,\n",
        "    'bert-large-cased': 512,\n",
        "    'bert-base-multilingual-uncased': 512,\n",
        "    'bert-base-multilingual-cased': 512,\n",
        "    'bert-base-chinese': 512,\n",
        "    'bert-base-german-cased': 512,\n",
        "    'bert-large-uncased-whole-word-masking': 512,\n",
        "    'bert-large-cased-whole-word-masking': 512,\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-base-cased-finetuned-mrpc': 512,\n",
        "}\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        tokens = reader.readlines()\n",
        "    for index, token in enumerate(tokens):\n",
        "        token = token.rstrip('\\n')\n",
        "        vocab[token] = index\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class BertTokenizer(PreTrainedTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a BertTokenizer.\n",
        "    :class:`~pytorch_transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
        "    Args:\n",
        "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
        "        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n",
        "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
        "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
        "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
        "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
        "            do_wordpiece_only=False\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n",
        "                 unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\",\n",
        "                 mask_token=\"[MASK]\", tokenize_chinese_chars=True, **kwargs):\n",
        "        \"\"\"Constructs a BertTokenizer.\n",
        "        Args:\n",
        "            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
        "            **do_lower_case**: (`optional`) boolean (default True)\n",
        "                Whether to lower case the input\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **do_basic_tokenize**: (`optional`) boolean (default True)\n",
        "                Whether to do basic tokenization before wordpiece.\n",
        "            **never_split**: (`optional`) list of string\n",
        "                List of tokens which will never be split during tokenization.\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n",
        "                                            pad_token=pad_token, cls_token=cls_token,\n",
        "                                            mask_token=mask_token, **kwargs)\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict(\n",
        "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.do_basic_tokenize = do_basic_tokenize\n",
        "        if do_basic_tokenize:\n",
        "            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                                  never_split=never_split,\n",
        "                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        if self.do_basic_tokenize:\n",
        "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
        "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                    split_tokens.append(sub_token)\n",
        "        else:\n",
        "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = ' '.join(tokens).replace(' ##', '').strip()\n",
        "        return out_string\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                                   \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n",
        "                    index = token_index\n",
        "                writer.write(token + u'\\n')\n",
        "                index += 1\n",
        "        return (vocab_file,)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        \"\"\" Instantiate a BertTokenizer from pre-trained vocabulary files.\n",
        "        \"\"\"\n",
        "        if pretrained_model_name_or_path in PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES:\n",
        "            if '-cased' in pretrained_model_name_or_path and kwargs.get('do_lower_case', True):\n",
        "                logger.warning(\"The pre-trained model you are loading is a cased model but you have not set \"\n",
        "                               \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"\n",
        "                               \"you may want to check this behavior.\")\n",
        "                kwargs['do_lower_case'] = False\n",
        "            elif '-cased' not in pretrained_model_name_or_path and not kwargs.get('do_lower_case', True):\n",
        "                logger.warning(\"The pre-trained model you are loading is an uncased model but you have set \"\n",
        "                               \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"\n",
        "                               \"but you may want to check this behavior.\")\n",
        "                kwargs['do_lower_case'] = True\n",
        "\n",
        "        return super(BertTokenizer, cls)._from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n",
        "        \"\"\" Constructs a BasicTokenizer.\n",
        "        Args:\n",
        "            **do_lower_case**: Whether to lower case the input.\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        if never_split is None:\n",
        "            never_split = []\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = never_split\n",
        "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
        "\n",
        "    def tokenize(self, text, never_split=None):\n",
        "        \"\"\" Basic Tokenization of a piece of text.\n",
        "            Split on \"white spaces\" only, for sub-word tokenization, see WordPieceTokenizer.\n",
        "        Args:\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "        \"\"\"\n",
        "        never_split = self.never_split + (never_split if never_split is not None else [])\n",
        "        text = self._clean_text(text)\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        if self.tokenize_chinese_chars:\n",
        "            text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in never_split:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text, never_split=None):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if never_split is not None and text in never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer`.\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVM8giu2BaDY"
      },
      "source": [
        "### Make imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4cwN8MqhGV9"
      },
      "source": [
        "import numpy as np\n",
        "from random import random, randrange, randint, shuffle, choice\n",
        "from tqdm import tqdm, trange\n",
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import fr_core_news_sm\n",
        "import yake\n",
        "import json\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9xZ1C0yBsvH"
      },
      "source": [
        "### Start CodeCarbon tracking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT1ze5rOBjM0",
        "outputId": "a8b70d04-c27a-48bc-b71f-1bccd6b5d439"
      },
      "source": [
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "# Initialise and start CodeCarbon tracker\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "# Initialise the Comet experiment\n",
        "experiment = Experiment(\n",
        "    api_key=\"XXXXXXXXXXXXXXXXXXX\",\n",
        "    project_name=\"general\",\n",
        "    workspace=\"xxxxxxx\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n",
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/asnota/general/c2e7d8774c4f4e4a8a38496bfe1d4851\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zb8gB3iAB0cw"
      },
      "source": [
        "### Create folders and initialize tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG9kTkrSBowv",
        "outputId": "e0d712a4-9134-46d5-8d71-02a357c1c7ea"
      },
      "source": [
        "# Create data folders\n",
        "def create_path(subdir):\n",
        "  if not os.path.exists(subdir):\n",
        "    os.makedirs(subdir)\n",
        "\n",
        "create_path('data_training')\n",
        "create_path('data_metrics')\n",
        "\n",
        "# Define device and model name\n",
        "device = \"cuda\"\n",
        "bert_model = \"bert-base-multilingual-uncased\"\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\n",
        "\n",
        "# Extend the vocabulary by adding a property to the tokenizer object\n",
        "tokenizer._noi_token = '[NOI]'\n",
        "tokenizer.vocab['[NOI]'] = tokenizer.vocab.pop('[unused1]')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 871891/871891 [00:00<00:00, 5561179.32B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmQbAZVHCjFX"
      },
      "source": [
        "### Define Document Database class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VslDCker8ZNm"
      },
      "source": [
        "class DocumentDatabase:\n",
        "    def __init__(self):\n",
        "      self.documents = []\n",
        "      self.document_shelf = None\n",
        "      self.document_shelf_filepath = None\n",
        "      self.temp_dir = None\n",
        "      self.doc_lengths = []\n",
        "      self.doc_cumsum = None\n",
        "      self.cumsum_max = None\n",
        "\n",
        "    # Public method to add a sequence to the documents list \n",
        "    # and it's length to the doc_length list\n",
        "    def add_document(self, document):\n",
        "      if not document:\n",
        "        return\n",
        "      self.documents.append(document)\n",
        "      self.doc_lengths.append(len(document))\n",
        "\n",
        "    # Private method to calculate cumulative sum based on the sequence length list \n",
        "    # and retrieve the max cumulative sum value\n",
        "    def _precalculate_doc_weights(self):\n",
        "      self.doc_cumsum = np.cumsum(self.doc_lengths)\n",
        "      self.cumsum_max = self.doc_cumsum[-1]\n",
        "\n",
        "    # Public method to sample sequences proportionally to their sequence length\n",
        "    def sample_doc(self, current_idx, sentence_weighted=True):\n",
        "      # Uses the current iteration counter to ensure we don't sample the same doc twice\n",
        "      if sentence_weighted:\n",
        "        # With sentence weighting, we sample docs proportionally to their sentence length\n",
        "        if self.doc_cumsum is None or len(self.doc_cumsum) != len(self.doc_lengths):\n",
        "          self._precalculate_doc_weights()\n",
        "        \n",
        "        rand_start = self.doc_cumsum[current_idx]\n",
        "        rand_end = rand_start + self.cumsum_max - self.doc_lengths[current_idx]\n",
        "        \n",
        "        sentence_index = randrange(rand_start, rand_end) % self.cumsum_max\n",
        "        sampled_doc_index = np.searchsorted(self.doc_cumsum, sentence_index, side='right')\n",
        "      else:\n",
        "        # If we don't use sentence weighting, then every doc has an equal chance to be chosen\n",
        "        sampled_doc_index = (current_idx + randrange(1, len(self.doc_lengths))) % len(self.doc_lengths)\n",
        "      assert sampled_doc_index != current_idx\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.doc_lengths)\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "      return self.documents[item]\n",
        "    \n",
        "    def __enter__(self):\n",
        "      return self\n",
        "    \n",
        "    def __exit__(self, exc_type, exc_val, traceback):\n",
        "      if self.document_shelf is not None:\n",
        "        self.document_shelf.close()\n",
        "      if self.temp_dir is not None:\n",
        "        self.temp_dir.cleanup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3LL9ZUmEwOf"
      },
      "source": [
        "## Define functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3svW7xDEfxL"
      },
      "source": [
        "### Truncate a sequence to a limited max size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvZnYE7FThIP"
      },
      "source": [
        "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
        "  \"\"\"Truncates a pair of sequences to a maximum sequence length. Lifted from Google's BERT repo.\"\"\"\n",
        "  \"Remove random truncate to remove last sentence\"\n",
        "  if len(tokens_a) <= max_num_tokens:\n",
        "    return\n",
        "      \n",
        "  # Find the index of a dot\n",
        "  indices = [i for i, x in enumerate(tokens_a) if x == \".\"]\n",
        "  if len(indices) > 0 and indices[-1] == len(tokens_a) - 1:\n",
        "    del indices[-1] # delete a dot\n",
        "      \n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b) # calculate total length for two tokens\n",
        "    if total_length <= max_num_tokens: # if this length is less than the max allowed length - leave it as it is\n",
        "      break\n",
        "        \n",
        "    trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b # return tokens with a bigger length\n",
        "    assert len(trunc_tokens) >= 1\n",
        "        \n",
        "    if len(indices) == 0: # if no dots found, leave as it is \n",
        "      return None\n",
        "    else:\n",
        "      del trunc_tokens[indices[-1]+1:]\n",
        "      del indices[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu3nNDOCFGX8"
      },
      "source": [
        "### Generate masked predictions and output them with tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kac9pxuu_d8z"
      },
      "source": [
        "def create_masked_lm_predictions(\n",
        "                        tokens, \n",
        "                        whole_word_mask, \n",
        "                        vocab_list, \n",
        "                        idf_dict, \n",
        "                        cls_token_at_end=False, \n",
        "                        pad_on_left=False,\n",
        "                        cls_token='[CLS]', \n",
        "                        sep_token='[SEP]', \n",
        "                        noi_token='[NOI]', \n",
        "                        pad_token=0,\n",
        "                        sequence_a_segment_id=0,\n",
        "                        cls_token_segment_id=1, \n",
        "                        pad_token_segment_id=0,\n",
        "                        mask_padding_with_zero=True,\n",
        "                        token_value='idf'):\n",
        "  \n",
        "  if token_value == 'idf':\n",
        "    prob_list =  np.array([idf_dict[t] for t in tokens])\n",
        "  else: # token_value == 'tf-idf' or token_value == 'tf-idf-stop':\n",
        "    tf = Counter(tokens)\n",
        "    tokens_len = float(len(tokens))\n",
        "    \n",
        "    # score: higher will be more likely to be keeped\n",
        "    prob_list =  np.array([idf_dict[t] * tf[t] / tokens_len for t in tokens])\n",
        "    \n",
        "  kw_extractor = yake.KeywordExtractor()\n",
        "  keywords = kw_extractor.extract_keywords(\" \".join(tokens))\n",
        "  key_word_len = 2 # /len(keywords)\n",
        " \n",
        "  for i, t in enumerate(tokens):\n",
        "    # Double the probability score for tokens that are keywords\n",
        "    for i, item in enumerate(keywords): \n",
        "\n",
        "      if len(prob_list) > i:\n",
        "        if item[0] == t:\n",
        "          prob_list[i] *= 2\n",
        "\n",
        "  # Check the repeated words and lower the probability, if it is the case\n",
        "  for i, t in enumerate(tokens):    \n",
        "    if t in tokens[:i]:\n",
        "      prob_list[i] /= 10 \n",
        "      \n",
        "\n",
        "  # prob_list: now, lower will be more likely to be keeped\n",
        "  prob_list = max(prob_list) - prob_list  \n",
        "\n",
        "  lm_label_tokens =  [noi_token] * len(tokens)\n",
        "  yield tokens, lm_label_tokens\n",
        "\n",
        "  origin_tokens = tokens.copy()\n",
        "  origin_prob_list = prob_list.copy()\n",
        "\n",
        "  skip_arg = 2 # minimal gap ranges\n",
        "  for skip in range(2,skip_arg+1):\n",
        "    N = len(origin_tokens)\n",
        "        \n",
        "    tokens = origin_tokens.copy()\n",
        "    prob_list = origin_prob_list.copy()\n",
        "    \n",
        "    while N > key_word_len + skip:\n",
        "      mask_pos = np.array(house_robber(prob_list, skip = skip))      \n",
        "      unmask_pos = np.setdiff1d(np.arange(N), mask_pos)\n",
        "      lm_label_tokens = ['[PAD]'] * len(unmask_pos)\n",
        "      j = 0\n",
        "      i = 1\n",
        "      while i < len(prob_list):\n",
        "        if i in mask_pos:\n",
        "          lm_label_tokens[j] = tokens[i]\n",
        "          i += 2\n",
        "        else:\n",
        "          lm_label_tokens[j] = noi_token\n",
        "          i += 1\n",
        "        j += 1\n",
        "      while j < len(unmask_pos):\n",
        "        lm_label_tokens[j] = noi_token # no input for last token of new sequence\n",
        "        j+= 1\n",
        "      \n",
        "      tokens = [t  for i,t in enumerate(tokens) if i in unmask_pos]\n",
        "      N = len(tokens)\n",
        "\n",
        "      prob_list = prob_list[unmask_pos] \n",
        "\n",
        "      yield tokens, lm_label_tokens\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEMPJi-pFZjs"
      },
      "source": [
        "### Add separators to sentence pairs (CLS, SEP and PAD)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTdCCP1W83kS"
      },
      "source": [
        "def create_instances_from_document(doc_database, doc_idx, max_seq_length, short_seq_prob, whole_word_mask, vocab_list, idf_dict,token_value=None, args=None):\n",
        "  \"\"\"This code is mostly a duplicate of the equivalent function from Google BERT's repo.\n",
        "    However, we make some changes and improvements. Sampling is improved and no longer requires a loop in this function.\n",
        "    Also, documents are sampled proportionally to the number of sentences they contain, which means each sentence\n",
        "    (rather than each document) has an equal chance of being sampled as a false example for the NextSentence task.\"\"\"\n",
        "  instances = []\n",
        "  document = doc_database[doc_idx]\n",
        "  # Account for [CLS], [SEP]\n",
        "  max_num_tokens = max_seq_length - 2\n",
        "\n",
        "  # We *usually* want to fill up the entire sequence since we are padding\n",
        "  # to `max_seq_length` anyways, so short sequences are generally wasted\n",
        "  # computation. However, we *sometimes*\n",
        "  # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
        "  # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
        "  # The `target_seq_length` is just a rough target however, whereas\n",
        "  # `max_seq_length` is a hard limit.\n",
        "  \n",
        "  target_seq_length = max_num_tokens\n",
        "  if random() < short_seq_prob:\n",
        "      target_seq_length = randint(2, max_num_tokens)\n",
        "    \n",
        "  tokens_a = document\n",
        "  truncate_seq_pair(tokens_a, [], max_num_tokens)\n",
        "\n",
        "  assert len(tokens_a) >= 1\n",
        "\n",
        "  tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + [\"[PAD]\"]  \n",
        "\n",
        "  for tokens, lm_label_tokens in create_masked_lm_predictions(tokens, whole_word_mask, vocab_list, idf_dict, token_value=token_value):    \n",
        "    instance = {\n",
        "                \"tokens\": tokens,\n",
        "                \"lm_label_tokens\": lm_label_tokens,\n",
        "                }\n",
        "    instances.append(instance)\n",
        "  return instances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Jf-vLfCFzri"
      },
      "source": [
        "### Form a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zosb31PoX2P"
      },
      "source": [
        "def cal_idf(idf_dict, docs, index_s, index_e, lock=None):\n",
        "  local_dict = {}\n",
        "  \n",
        "  for i in trange(index_s, index_e, ): \n",
        "    # Add CLS, SEP and PAD \n",
        "    tokens= [\"[CLS]\"] + docs[i] + [\"[SEP]\"] + [\"[PAD]\"]\n",
        "    for t in tokens:   \n",
        "      local_dict[t] = local_dict.get(t, 0) + 1\n",
        "\n",
        "    for k, v in local_dict.items():\n",
        "            idf_dict[k] = idf_dict.get(k,0) + v            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t3EQh4NGgK2"
      },
      "source": [
        "### Clean a text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q0QUzC_AJMa"
      },
      "source": [
        "def clean_str(txt):\n",
        "  txt = txt.lower()\n",
        "  \n",
        "  # Add a space in the beggining of the phrase\n",
        "  txt = re.sub('^',' ', txt)\n",
        "\n",
        "  # Add a space in the end of the phrase\n",
        "  txt = re.sub('$',' ', txt)\n",
        "\n",
        "  # Replace http by __url__ \n",
        "  words = []\n",
        "  for word in txt.split():   \n",
        "    i = word.find('http')   \n",
        "    if i >=0:\n",
        "      word = word[:i] + ' ' + '__url__'\n",
        "    words.append(word.strip())\n",
        "  txt = ' '.join(words)\n",
        "\n",
        "  # Add space after and before a dot and get rid of triple dot\n",
        "  words = []\n",
        "  for word in txt.split():      \n",
        "    dot = word.find('.')    \n",
        "    if dot >=0:\n",
        "      \n",
        "      if dot+1 == len(word):       \n",
        "        word = word[0:dot] + ' ' + word[dot]\n",
        "\n",
        "      elif word[dot+1] == word[dot]:        \n",
        "        substr1 = word[0:dot]     \n",
        "        substr2 = word[dot + 3:len(word)]\n",
        "        word  = substr1 + ' ' + word[dot] + ' ' + substr2        \n",
        "     \n",
        "      else:\n",
        "        substr1 = word[0:dot] \n",
        "        substr2 = word[dot + 1:len(word)]\n",
        "        word  = substr1 + ' ' + word[dot] + ' ' + substr2        \n",
        "    words.append(word.strip())  \n",
        "  txt = ' '.join(words)\n",
        "  \n",
        "  # Remove markdown URL\n",
        "  txt = re.sub(r'\\[([^\\]]*)\\] \\( *__url__ *\\)', r'\\1', txt)\n",
        "\n",
        "  # Remove alphanumeric characters\n",
        "  txt = txt.replace(')', '')\n",
        "  txt = txt.replace('(', '')\n",
        "  txt = txt.replace('\"', '')\n",
        "  txt = txt.replace('', '')\n",
        "  \n",
        "  # Remove all string breaks\n",
        "  txt = re.sub(r'^\\s+', '', txt)\n",
        "  txt = re.sub(r'\\s+$', '', txt)\n",
        "  txt = re.sub(r'\\s+', ' ', txt)\n",
        " \n",
        "  return txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAn5-0CjGpoN"
      },
      "source": [
        "### Compute the chunk size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDg2duTX3538"
      },
      "source": [
        "def partitionIndexes(totalsize, numberofpartitions):\n",
        "    # Compute the chunk size (integer division; i.e. assuming Python 2.7)\n",
        "    chunksize = int(totalsize / numberofpartitions)\n",
        "    # How many chunks need an extra 1 added to the size?\n",
        "    remainder = totalsize - chunksize * numberofpartitions\n",
        "    a = 0\n",
        "    for i in range(numberofpartitions):\n",
        "        b = a + chunksize + (i < remainder)\n",
        "        # Yield the inclusive-inclusive range\n",
        "        yield (a, b )\n",
        "        a = b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMXsK_X4NksB"
      },
      "source": [
        "### Calculate a mask position for prob_list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkfbOBgvmNDF"
      },
      "source": [
        "def house_robber(prob_list, skip = 2):\n",
        "  pos = [0] * len(prob_list)\n",
        "  count = [0] * len(prob_list)\n",
        "  pos[0] = []\n",
        "  count[0] = 0.\n",
        "  \n",
        "  if len(prob_list) <= skip:\n",
        "      return [np.argmax(prob_list) + 1]\n",
        "  \n",
        "  for s in range(1,skip):\n",
        "      pos[s] = [s]\n",
        "      count[s] = prob_list[s]\n",
        "\n",
        "  for i in range(skip, len(prob_list)):\n",
        "      if prob_list[i] + count[i-skip] > max([count[i-j] for j in range(1,skip)]):\n",
        "          pos[i] = pos[i-skip].copy()\n",
        "          pos[i].append(i)\n",
        "          count[i] = prob_list[i] + count[i-skip]\n",
        "      else:\n",
        "          max_id = np.argmax([count[i-j] for j in range(1,skip)]) + 1\n",
        "          pos[i] = pos[i-max_id].copy()\n",
        "          count[i] = count[i-max_id]\n",
        "  return pos[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icQqiFTEN0sT"
      },
      "source": [
        "### Create folders to store the result of the pretraining data generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GaHqn5sSQ_u"
      },
      "source": [
        "def create_folders(epoch_num):  \n",
        "  epoch_filename = \"data_training/file_\" + \"epoch_{}.json\".format(epoch_num)\n",
        "  metrics_file = \"data_metrics/metrics_\" + \"epoch_{}.json\".format(epoch_num)\n",
        "\n",
        "  f = open(epoch_filename, \"x\")\n",
        "  f = open(metrics_file, \"x\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu11ZVAMOKmO"
      },
      "source": [
        "### Create training files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MCLHved4Xxb"
      },
      "source": [
        "CUTOFF = 5\n",
        "max_seq_len = 265\n",
        "short_seq_prob=10\n",
        "do_whole_word_mask=False\n",
        "\n",
        "def create_training_file(docs, vocab_list, epoch_num, index_s, index_e, idf_dict, token_value): \n",
        "  epoch_filename = \"data_training/file_\" + \"epoch_{}.json\".format(epoch_num)\n",
        "  metrics_file = \"data_metrics/metrics_\" + \"epoch_{}.json\".format(epoch_num)\n",
        "    \n",
        "  num_instances = 0\n",
        "\n",
        "  with open(epoch_filename, 'w') as epoch_file:\n",
        "    for doc_idx in trange(index_s, index_e, desc=\"Document\"):\n",
        "      if len(docs[doc_idx]) <= CUTOFF: continue\n",
        "      \n",
        "      doc_instances = create_instances_from_document(docs, doc_idx, max_seq_length=max_seq_len, short_seq_prob=short_seq_prob,\n",
        "                whole_word_mask=do_whole_word_mask, vocab_list=vocab_list, idf_dict=idf_dict, token_value=token_value)\n",
        "      \n",
        "      doc_instances = [json.dumps(instance, ensure_ascii=False) for instance in doc_instances]     \n",
        "      \n",
        "      for instance in doc_instances:\n",
        "        epoch_file.write(instance + '\\n')\n",
        "        num_instances += 1\n",
        "\n",
        "  with open(metrics_file, 'w') as metrics_file:\n",
        "    metrics = {\n",
        "                \"num_training_examples\": num_instances,\n",
        "                \"max_seq_len\": max_seq_len\n",
        "              }\n",
        "    metrics_file.write(json.dumps(metrics))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e39SgWFsQz9J"
      },
      "source": [
        "## Get datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsvAUTwAaity",
        "outputId": "2cf7ba16-87b0-4871-a281-0d4152b70b9a"
      },
      "source": [
        "# Run for CC100 dataset chunk retrieval\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"sample_data/xan.txt\"\n",
        "txtObj = pd.read_csv(file_path, delimiter = \"\\t\", header=None)\n",
        "\n",
        "TXTtext = txtObj[0].to_list()\n",
        "len(TXTtext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64208"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oio4cS1ADiHm"
      },
      "source": [
        "## Main thread"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n4juMLCE8jGU"
      },
      "source": [
        " from datetime import datetime\n",
        " start_time = datetime.now()\n",
        "\n",
        " train_corpus = TXTtext\n",
        "\n",
        " max_line = len(train_corpus)\n",
        " epochs_to_generate = 3\n",
        " token_value = \"df-stop\"\n",
        " \n",
        " with DocumentDatabase() as docs:\n",
        "    doc = []\n",
        "    lines_to_add = []\n",
        "    iterations = 0\n",
        "    \n",
        "    for line in tqdm(train_corpus, desc=\"Loading Dataset\", unit=\" lines\"):\n",
        "      iterations += 1\n",
        "      if max_line and iterations >= max_line:\n",
        "        break\n",
        "      line = line.strip()\n",
        "             \n",
        "      if line == \"\" or len(line.split()) <=1:\n",
        "        continue\n",
        "      else:\n",
        "        line = clean_str(line)\n",
        "        lines_to_add.append(line)       \n",
        "       \n",
        "    for line in tqdm(lines_to_add):\n",
        "      tokens = line.split()\n",
        "      \n",
        "      docs.add_document(tokens)\n",
        "\n",
        "    if len(docs) <= 1:\n",
        "      exit(\"ERROR: No document breaks were found in the input file! These are necessary to allow the script to \"\n",
        "            \"ensure that random NextSentences are not sampled from the same document. Please add blank lines to \"\n",
        "            \"indicate breaks between documents in your input file. If your dataset does not contain multiple \"\n",
        "            \"documents, blank lines can be inserted at any natural boundary, such as the ends of chapters, \"\n",
        "            \"sections or paragraphs.\")\n",
        "      \n",
        "    idf_dict = {}\n",
        "    cal_idf(idf_dict, docs, 0, len(docs))\n",
        "    \n",
        "    docs_len = float(len(docs))\n",
        "    \n",
        "    for t in idf_dict.keys():\n",
        "      idf_dict[t] = np.log(docs_len / idf_dict[t] )\n",
        "\n",
        "\n",
        "    if token_value == \"df-stop\":\n",
        "        \n",
        "      stop_words = set(stopwords.words('french') ) | set(['[SEP]', '[PAD]', '[CLS]', '', 'de', 'en', 't', 'est', \"ont\" \"eu\", \"a\" '\"', 'pour', 'sur', 'comme', 'avec', 'par', 'lui', 'prs', 'a', 'quel', '.', ',', '(', ')',\"'\", '%'])\n",
        "      \n",
        "      # Make all the values be less than 1, in other words - normalize the values between 0 and 1\n",
        "      for k in idf_dict.keys():\n",
        "        idf_dict[k] = 1.0/(idf_dict[k] + 1e-5)\n",
        "\n",
        "      # Penalize stopwords occurencies, making then lower than 0.0...\n",
        "      for t in stop_words:\n",
        "        if t in idf_dict:\n",
        "          idf_dict[t] = 0.01/(idf_dict[t])\n",
        "\n",
        "      def hasNumbers(inputString):\n",
        "        return any(char.isdigit() for char in inputString)\n",
        "      \n",
        "      inp = \" \".join([k for k in idf_dict.keys() if not hasNumbers(k)])\n",
        "      spacy_nlp = spacy.load('fr_core_news_sm')\n",
        "      inp_results = [(token.text, token.tag_) for token in spacy_nlp(inp[:1000000])]\n",
        "      \n",
        "      allowed_tags = ['VERB','NOUN','ADJ_','ADV_']\n",
        "      ignored_words = ['t','a','avait','aurait','serait'] + ['du','pu','fait'] # verbs of no info\n",
        "\n",
        "      # Calculate idf score\n",
        "      for word, tag in inp_results:\n",
        "        if word in idf_dict.keys():\n",
        "          if len(tag)>=2 and tag[:4] in allowed_tags and (word not in ignored_words):\n",
        "            if tag[:4] in ['VERB','NOUN']:\n",
        "              idf_dict[word] *= 4\n",
        "            else:\n",
        "              idf_dict[word] *= 2\n",
        "      \n",
        "      token_value = token_value\n",
        "                 \n",
        "\n",
        "    idx_list = [i for i in partitionIndexes(len(docs), epochs_to_generate)]\n",
        "\n",
        "    for epoch in trange(epochs_to_generate, desc=\"Epoch\"):\n",
        "      create_folders(epoch)\n",
        "      create_training_file(docs, vocab_list, epoch, idx_list[epoch][0], idx_list[epoch][1], idf_dict, token_value) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cviRGS5vTtpq"
      },
      "source": [
        "### Stop CO2 tracker and stop comet experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wDCsCokErso"
      },
      "source": [
        "# Stop CO2 tracker and print emissions\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions: {emissions} kg\")\n",
        "\n",
        "# Calculate the time spent\n",
        "stop_time = datetime.now() - start_time\n",
        "\n",
        "# Log the time to Comet\n",
        "hyper_params = {\n",
        "    \"time spent\": stop_time,\n",
        "    \"emmissions\": emissions\n",
        "}\n",
        "experiment.log_parameters(hyper_params)\n",
        "\n",
        "# Turn off Comet\n",
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1aYGeghT2l5"
      },
      "source": [
        "### Form zip files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqhDNXek_5Gc"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(path, '..')))\n",
        "\n",
        "zipf = zipfile.ZipFile('training_data.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('data_training', zipf)\n",
        "\n",
        "zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WKIel-kK5CN"
      },
      "source": [
        "zipf_metrics = zipfile.ZipFile('metrics_data.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('data_metrics', zipf_metrics)\n",
        "\n",
        "zipf_metrics.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "vWq3qfa0Csn1",
        "outputId": "a5f0d1f6-b53d-4bd1-d520-ab74596d3549"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('training_data.zip')\n",
        "files.download('metrics_data.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_4062e5d0-277c-4d07-be91-93599f6c746d\", \"training_data.zip\", 12854801)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_82364d96-7b23-4050-bb06-248c5fe1f0b2\", \"metrics_data.zip\", 607)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}