{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDGsVQzI83Rd"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ2zz5G104QB"
      },
      "source": [
        "%pip install boto3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkaXCJI_87A-"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQpCdiR90qR_",
        "outputId": "41e77642-ee19-4614-ae43-4b45b988b065"
      },
      "source": [
        "# Cached_path\n",
        "\"\"\"\n",
        "Utilities for working with the local dataset cache.\n",
        "This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\n",
        "Copyright by the AllenNLP authors.\n",
        "\"\"\"\n",
        "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import fnmatch\n",
        "from functools import wraps\n",
        "from hashlib import sha256\n",
        "from io import open\n",
        "\n",
        "import boto3\n",
        "import requests\n",
        "from botocore.exceptions import ClientError\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from torch.hub import _get_torch_home\n",
        "    torch_cache_home = _get_torch_home()\n",
        "except ImportError:\n",
        "    torch_cache_home = os.path.expanduser(\n",
        "        os.getenv('TORCH_HOME', os.path.join(\n",
        "            os.getenv('XDG_CACHE_HOME', '~/.cache'), 'torch')))\n",
        "default_cache_path = os.path.join(torch_cache_home, 'pytorch_transformers')\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "\n",
        "try:\n",
        "    from pathlib import Path\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
        "        os.getenv('PYTORCH_TRANSFORMERS_CACHE', os.getenv('PYTORCH_PRETRAINED_BERT_CACHE', default_cache_path)))\n",
        "except (AttributeError, ImportError):\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_TRANSFORMERS_CACHE',\n",
        "                                              os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                                        default_cache_path))\n",
        "\n",
        "PYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "def url_to_filename(url, etag=None):\n",
        "    \"\"\"\n",
        "    Convert `url` into a hashed filename in a repeatable way.\n",
        "    If `etag` is specified, append its hash to the url's, delimited\n",
        "    by a period.\n",
        "    \"\"\"\n",
        "    url_bytes = url.encode('utf-8')\n",
        "    url_hash = sha256(url_bytes)\n",
        "    filename = url_hash.hexdigest()\n",
        "\n",
        "    if etag:\n",
        "        etag_bytes = etag.encode('utf-8')\n",
        "        etag_hash = sha256(etag_bytes)\n",
        "        filename += '.' + etag_hash.hexdigest()\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def filename_to_url(filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
        "    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "    if not os.path.exists(cache_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
        "\n",
        "    meta_path = cache_path + '.json'\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
        "\n",
        "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
        "        metadata = json.load(meta_file)\n",
        "    url = metadata['url']\n",
        "    etag = metadata['etag']\n",
        "\n",
        "    return url, etag\n",
        "\n",
        "\n",
        "def cached_path(url_or_filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given something that might be a URL (or might be a local path),\n",
        "    determine which. If it's a URL, download the file and cache it, and\n",
        "    return the path to the cached file. If it's already a local path,\n",
        "    make sure the file exists and then return the path.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n",
        "        url_or_filename = str(url_or_filename)\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    parsed = urlparse(url_or_filename)\n",
        "\n",
        "    if parsed.scheme in ('http', 'https', 's3'):\n",
        "        # URL, so get it from the cache (downloading if necessary)\n",
        "        return get_from_cache(url_or_filename, cache_dir)\n",
        "    elif os.path.exists(url_or_filename):\n",
        "        # File, and it exists.\n",
        "        return url_or_filename\n",
        "    elif parsed.scheme == '':\n",
        "        # File, but it doesn't exist.\n",
        "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
        "    else:\n",
        "        # Something unknown\n",
        "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
        "\n",
        "\n",
        "def split_s3_path(url):\n",
        "    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.netloc or not parsed.path:\n",
        "        raise ValueError(\"bad s3 path {}\".format(url))\n",
        "    bucket_name = parsed.netloc\n",
        "    s3_path = parsed.path\n",
        "    # Remove '/' at beginning of path.\n",
        "    if s3_path.startswith(\"/\"):\n",
        "        s3_path = s3_path[1:]\n",
        "    return bucket_name, s3_path\n",
        "\n",
        "\n",
        "def s3_request(func):\n",
        "    \"\"\"\n",
        "    Wrapper function for s3 requests in order to create more helpful error\n",
        "    messages.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(url, *args, **kwargs):\n",
        "        try:\n",
        "            return func(url, *args, **kwargs)\n",
        "        except ClientError as exc:\n",
        "            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n",
        "                raise EnvironmentError(\"file {} not found\".format(url))\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_etag(url):\n",
        "    \"\"\"Check ETag on S3 object.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_object = s3_resource.Object(bucket_name, s3_path)\n",
        "    return s3_object.e_tag\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_get(url, temp_file):\n",
        "    \"\"\"Pull a file directly from S3.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n",
        "\n",
        "\n",
        "def http_get(url, temp_file):\n",
        "    req = requests.get(url, stream=True)\n",
        "    content_length = req.headers.get('Content-Length')\n",
        "    total = int(content_length) if content_length is not None else None\n",
        "    progress = tqdm(unit=\"B\", total=total)\n",
        "    for chunk in req.iter_content(chunk_size=1024):\n",
        "        if chunk: # filter out keep-alive new chunks\n",
        "            progress.update(len(chunk))\n",
        "            temp_file.write(chunk)\n",
        "    progress.close()\n",
        "\n",
        "\n",
        "def get_from_cache(url, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given a URL, look for the corresponding dataset in the local cache.\n",
        "    If it's not there, download it. Then return the path to the cached file.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "    if sys.version_info[0] == 2 and not isinstance(cache_dir, str):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    # Get eTag to add to filename, if it exists.\n",
        "    if url.startswith(\"s3://\"):\n",
        "        etag = s3_etag(url)\n",
        "    else:\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True)\n",
        "            if response.status_code != 200:\n",
        "                etag = None\n",
        "            else:\n",
        "                etag = response.headers.get(\"ETag\")\n",
        "        except EnvironmentError:\n",
        "            etag = None\n",
        "\n",
        "    if sys.version_info[0] == 2 and etag is not None:\n",
        "        etag = etag.decode('utf-8')\n",
        "    filename = url_to_filename(url, etag)\n",
        "\n",
        "    # get cache path to put the file\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "\n",
        "    # If we don't have a connection (etag is None) and can't identify the file\n",
        "    # try to get the last downloaded one\n",
        "    if not os.path.exists(cache_path) and etag is None:\n",
        "        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + '.*')\n",
        "        matching_files = list(filter(lambda s: not s.endswith('.json'), matching_files))\n",
        "        if matching_files:\n",
        "            cache_path = os.path.join(cache_dir, matching_files[-1])\n",
        "\n",
        "    if not os.path.exists(cache_path):\n",
        "        # Download to temporary file, then copy to cache dir once finished.\n",
        "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
        "        with tempfile.NamedTemporaryFile() as temp_file:\n",
        "            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n",
        "\n",
        "            # GET file object\n",
        "            if url.startswith(\"s3://\"):\n",
        "                s3_get(url, temp_file)\n",
        "            else:\n",
        "                http_get(url, temp_file)\n",
        "\n",
        "            # we are copying the file before closing it, so flush to avoid truncation\n",
        "            temp_file.flush()\n",
        "            # shutil.copyfileobj() starts at the current position, so go to the start\n",
        "            temp_file.seek(0)\n",
        "\n",
        "            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n",
        "            with open(cache_path, 'wb') as cache_file:\n",
        "                shutil.copyfileobj(temp_file, cache_file)\n",
        "\n",
        "            logger.info(\"creating metadata file for %s\", cache_path)\n",
        "            meta = {'url': url, 'etag': etag}\n",
        "            meta_path = cache_path + '.json'\n",
        "            with open(meta_path, 'w') as meta_file:\n",
        "                output_string = json.dumps(meta)\n",
        "                if sys.version_info[0] == 2 and isinstance(output_string, str):\n",
        "                    output_string = unicode(output_string, 'utf-8')  # The beauty of python 2\n",
        "                meta_file.write(output_string)\n",
        "\n",
        "            logger.info(\"removing temp file %s\", temp_file.name)\n",
        "\n",
        "    return cache_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.5) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5wenp7R0isX"
      },
      "source": [
        "# WEIGHTS_NAME, CONFIG_NAME, PretrainedConfig, PreTrainedModel, prune_linear_layer, add_start_docstrings\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import (absolute_import, division, print_function,\n",
        "                        unicode_literals)\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from io import open\n",
        "\n",
        "import six\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import functional as F\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "TF_WEIGHTS_NAME = 'model.ckpt'\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.nn import Identity\n",
        "except ImportError:\n",
        "    # Older PyTorch compatibility\n",
        "    class Identity(nn.Module):\n",
        "        r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
        "        \"\"\"\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super(Identity, self).__init__()\n",
        "\n",
        "        def forward(self, input):\n",
        "            return input\n",
        "\n",
        "\n",
        "if not six.PY2:\n",
        "    def add_start_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            fn.__doc__ = ''.join(docstr) + fn.__doc__\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "else:\n",
        "    # Not possible to update class docstrings on python2\n",
        "    def add_start_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "\n",
        "\n",
        "class PretrainedConfig(object):\n",
        "    r\"\"\" Base class for all configuration classes.\n",
        "        Handles a few parameters common to all models' configurations as well as methods for loading/downloading/saving configurations.\n",
        "        Class attributes (overridden by derived classes):\n",
        "            - ``pretrained_config_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained model configurations as values.\n",
        "        Parameters:\n",
        "            ``finetuning_task``: string, default `None`. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n",
        "            ``num_labels``: integer, default `2`. Number of classes to use when the model is a classification model (sequences/tokens)\n",
        "            ``output_attentions``: boolean, default `False`. Should the model returns attentions weights.\n",
        "            ``output_hidden_states``: string, default `False`. Should the model returns all hidden-states.\n",
        "            ``torchscript``: string, default `False`. Is the model used with Torchscript.\n",
        "    \"\"\"\n",
        "    pretrained_config_archive_map = {}\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.finetuning_task = kwargs.pop('finetuning_task', None)\n",
        "        self.num_labels = kwargs.pop('num_labels', 2)\n",
        "        self.output_attentions = kwargs.pop('output_attentions', False)\n",
        "        self.output_hidden_states = kwargs.pop('output_hidden_states', False)\n",
        "        self.torchscript = kwargs.pop('torchscript', False)\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save a configuration object to the directory `save_directory`, so that it\n",
        "            can be re-loaded using the :func:`~pytorch_transformers.PretrainedConfig.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n",
        "\n",
        "        # If we save using the predefined names, we can load using `from_pretrained`\n",
        "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n",
        "\n",
        "        self.to_json_file(output_config_file)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n",
        "        r\"\"\" Instantiate a :class:`~pytorch_transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n",
        "        Parameters:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a path to a `directory` containing a configuration file saved using the :func:`~pytorch_transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
        "                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded pre-trained model\n",
        "                configuration should be cached if the standard cache should not be used.\n",
        "            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n",
        "                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n",
        "                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n",
        "            return_unused_kwargs: (`optional`) bool:\n",
        "                - If False, then this function returns just the final configuration object.\n",
        "                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n",
        "        Examples::\n",
        "            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n",
        "            # derived class: BertConfig\n",
        "            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n",
        "            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
        "            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n",
        "            config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n",
        "            assert config.output_attention == True\n",
        "            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n",
        "                                                               foo=False, return_unused_kwargs=True)\n",
        "            assert config.output_attention == True\n",
        "            assert unused_kwargs == {'foo': False}\n",
        "        \"\"\"\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\n",
        "        return_unused_kwargs = kwargs.pop('return_unused_kwargs', False)\n",
        "\n",
        "        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n",
        "            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\n",
        "        elif os.path.isdir(pretrained_model_name_or_path):\n",
        "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
        "        else:\n",
        "            config_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_config_file = cached_path(config_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n",
        "                logger.error(\n",
        "                    \"Couldn't reach server at '{}' to download pretrained model configuration file.\".format(\n",
        "                        config_file))\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                    \"associated to this path or url.\".format(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        ', '.join(cls.pretrained_config_archive_map.keys()),\n",
        "                        config_file))\n",
        "            return None\n",
        "        if resolved_config_file == config_file:\n",
        "            logger.info(\"loading configuration file {}\".format(config_file))\n",
        "        else:\n",
        "            logger.info(\"loading configuration file {} from cache at {}\".format(\n",
        "                config_file, resolved_config_file))\n",
        "\n",
        "        # Load config\n",
        "        config = cls.from_json_file(resolved_config_file)\n",
        "\n",
        "        # Update config with kwargs if needed\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(config, key):\n",
        "                setattr(config, key, value)\n",
        "                to_remove.append(key)\n",
        "        for key in to_remove:\n",
        "            kwargs.pop(key, None)\n",
        "\n",
        "        logger.info(\"Model config %s\", config)\n",
        "        if return_unused_kwargs:\n",
        "            return config, kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"\n",
        "        config = cls(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.__dict__ == other.__dict__\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "    def to_json_file(self, json_file_path):\n",
        "        \"\"\" Save this instance to a json file.\"\"\"\n",
        "        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n",
        "            writer.write(self.to_json_string())\n",
        "\n",
        "\n",
        "class PreTrainedModel(nn.Module):\n",
        "    r\"\"\" Base class for all models.\n",
        "        :class:`~pytorch_transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models\n",
        "        as well as a few methods commons to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.\n",
        "        Class attributes (overridden by derived classes):\n",
        "            - ``config_class``: a class derived from :class:`~pytorch_transformers.PretrainedConfig` to use as configuration class for this model architecture.\n",
        "            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.\n",
        "            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:\n",
        "                - ``model``: an instance of the relevant subclass of :class:`~pytorch_transformers.PreTrainedModel`,\n",
        "                - ``config``: an instance of the relevant subclass of :class:`~pytorch_transformers.PretrainedConfig`,\n",
        "                - ``path``: a path (string) to the TensorFlow checkpoint.\n",
        "            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.\n",
        "    \"\"\"\n",
        "    config_class = None\n",
        "    pretrained_model_archive_map = {}\n",
        "    load_tf_weights = lambda model, config, path: None\n",
        "    base_model_prefix = \"\"\n",
        "\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedModel, self).__init__()\n",
        "        if not isinstance(config, PretrainedConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"\n",
        "                \"To create a model from a pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        # Save config in model\n",
        "        self.config = config\n",
        "\n",
        "    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\n",
        "        \"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n",
        "            Increasing the size will add newly initialized vectors at the end\n",
        "            Reducing the size will remove vectors from the end\n",
        "        Args:\n",
        "            new_num_tokens: (`optional`) int\n",
        "                New number of tokens in the embedding matrix.\n",
        "                Increasing the size will add newly initialized vectors at the end\n",
        "                Reducing the size will remove vectors from the end\n",
        "                If not provided or None: return the provided token Embedding Module.\n",
        "        Return: ``torch.nn.Embeddings``\n",
        "            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n",
        "        \"\"\"\n",
        "        if new_num_tokens is None:\n",
        "            return old_embeddings\n",
        "\n",
        "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "        if old_num_tokens == new_num_tokens:\n",
        "            return old_embeddings\n",
        "\n",
        "        # Build new embeddings\n",
        "        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n",
        "        new_embeddings.to(old_embeddings.weight.device)\n",
        "\n",
        "        # initialize all new embeddings (in particular added tokens)\n",
        "        self.init_weights(new_embeddings)\n",
        "\n",
        "        # Copy word embeddings from the previous weights\n",
        "        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n",
        "        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n",
        "\n",
        "        return new_embeddings\n",
        "\n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n",
        "        \"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens=None):\n",
        "        \"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n",
        "        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n",
        "        Arguments:\n",
        "            new_num_tokens: (`optional`) int:\n",
        "                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end. \n",
        "                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
        "        Return: ``torch.nn.Embeddings``\n",
        "            Pointer to the input tokens Embeddings Module of the model\n",
        "        \"\"\"\n",
        "        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n",
        "        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\n",
        "        if new_num_tokens is None:\n",
        "            return model_embeds\n",
        "\n",
        "        # Update base model and current model config\n",
        "        self.config.vocab_size = new_num_tokens\n",
        "        base_model.vocab_size = new_num_tokens\n",
        "\n",
        "        # Tie weights again if needed\n",
        "        if hasattr(self, 'tie_weights'):\n",
        "            self.tie_weights()\n",
        "\n",
        "        return model_embeds\n",
        "\n",
        "    def prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the base model.\n",
        "            Arguments:\n",
        "                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n",
        "        \"\"\"\n",
        "        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n",
        "        base_model._prune_heads(heads_to_prune)\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save a model and its configuration file to a directory, so that it\n",
        "            can be re-loaded using the `:func:`~pytorch_transformers.PreTrainedModel.from_pretrained`` class method.\n",
        "        \"\"\"\n",
        "        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n",
        "\n",
        "        # Only save the model it-self if we are using distributed training\n",
        "        model_to_save = self.module if hasattr(self, 'module') else self\n",
        "\n",
        "        # Save configuration file\n",
        "        model_to_save.config.save_pretrained(save_directory)\n",
        "\n",
        "        # If we save using the predefined names, we can load using `from_pretrained`\n",
        "        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n",
        "\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n",
        "        r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
        "        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n",
        "        To train the model, you should first set it back in training mode with ``model.train()``\n",
        "        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n",
        "        It is up to you to train those weights with a downstream fine-tuning task.\n",
        "        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n",
        "        Parameters:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a path to a `directory` containing model weights saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n",
        "                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
        "            model_args: (`optional`) Sequence of positional arguments:\n",
        "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n",
        "            config: (`optional`) instance of a class derived from :class:`~pytorch_transformers.PretrainedConfig`:\n",
        "                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n",
        "                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n",
        "                - the model was saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n",
        "                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n",
        "            state_dict: (`optional`) dict:\n",
        "                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n",
        "                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n",
        "                In this case though, you should check if using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and :func:`~pytorch_transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded pre-trained model\n",
        "                configuration should be cached if the standard cache should not be used.\n",
        "            output_loading_info: (`optional`) boolean:\n",
        "                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n",
        "            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
        "                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n",
        "                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n",
        "                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~pytorch_transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n",
        "        Examples::\n",
        "            model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n",
        "            model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
        "            model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n",
        "            assert model.config.output_attention == True\n",
        "            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
        "            config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n",
        "            model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n",
        "        \"\"\"\n",
        "        config = kwargs.pop('config', None)\n",
        "        state_dict = kwargs.pop('state_dict', None)\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\n",
        "        from_tf = kwargs.pop('from_tf', False)\n",
        "        output_loading_info = kwargs.pop('output_loading_info', False)\n",
        "\n",
        "        # Load config\n",
        "        if config is None:\n",
        "            config, model_kwargs = cls.config_class.from_pretrained(\n",
        "                pretrained_model_name_or_path, *model_args,\n",
        "                cache_dir=cache_dir, return_unused_kwargs=True,\n",
        "                **kwargs\n",
        "            )\n",
        "        else:\n",
        "            model_kwargs = kwargs\n",
        "\n",
        "        # Load model\n",
        "        if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n",
        "            archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\n",
        "        elif os.path.isdir(pretrained_model_name_or_path):\n",
        "            if from_tf:\n",
        "                # Directly load from a TensorFlow checkpoint\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + \".index\")\n",
        "            else:\n",
        "                archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n",
        "        else:\n",
        "            if from_tf:\n",
        "                # Directly load from a TensorFlow checkpoint\n",
        "                archive_file = pretrained_model_name_or_path + \".index\"\n",
        "            else:\n",
        "                archive_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n",
        "                logger.error(\n",
        "                    \"Couldn't reach server at '{}' to download pretrained weights.\".format(\n",
        "                        archive_file))\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                    \"associated to this path or url.\".format(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        ', '.join(cls.pretrained_model_archive_map.keys()),\n",
        "                        archive_file))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading weights file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading weights file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *model_args, **model_kwargs)\n",
        "\n",
        "        if state_dict is None and not from_tf:\n",
        "            state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
        "        if from_tf:\n",
        "            # Directly load from a TensorFlow checkpoint\n",
        "            return cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the '.index'\n",
        "\n",
        "        # Convert old format to new format if needed from a PyTorch state_dict\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        # Load from a PyTorch state_dict\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "\n",
        "        # Make sure we are able to load base models as well as derived models (with heads)\n",
        "        start_prefix = ''\n",
        "        model_to_load = model\n",
        "        if not hasattr(model, cls.base_model_prefix) and any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n",
        "            start_prefix = cls.base_model_prefix + '.'\n",
        "        if hasattr(model, cls.base_model_prefix) and not any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n",
        "            model_to_load = getattr(model, cls.base_model_prefix)\n",
        "\n",
        "        load(model_to_load, prefix=start_prefix)\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if len(error_msgs) > 0:\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "\n",
        "        if hasattr(model, 'tie_weights'):\n",
        "            model.tie_weights()  # make sure word embedding weights are still tied\n",
        "\n",
        "        # Set model in evaluation mode to desactivate DropOut modules by default\n",
        "        model.eval()\n",
        "\n",
        "        if output_loading_info:\n",
        "            loading_info = {\"missing_keys\": missing_keys, \"unexpected_keys\": unexpected_keys, \"error_msgs\": error_msgs}\n",
        "            return model, loading_info\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n",
        "            Basically works like a Linear layer but the weights are transposed\n",
        "        \"\"\"\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerStartLogits(nn.Module):\n",
        "    \"\"\" Compute SQuAD start_logits from sequence hidden states. \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerStartLogits, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states, p_mask=None):\n",
        "        \"\"\" Args:\n",
        "            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n",
        "                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n",
        "                1.0 means token should be masked.\n",
        "        \"\"\"\n",
        "        x = self.dense(hidden_states).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerEndLogits(nn.Module):\n",
        "    \"\"\" Compute SQuAD end_logits from sequence hidden states and start token hidden state.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerEndLogits, self).__init__()\n",
        "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n",
        "        \"\"\" Args:\n",
        "            One of ``start_states``, ``start_positions`` should be not None.\n",
        "            If both are set, ``start_positions`` overrides ``start_states``.\n",
        "            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n",
        "                hidden states of the first tokens for the labeled span.\n",
        "            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "                position of the first token for the labeled span:\n",
        "            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n",
        "                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n",
        "                1.0 means token should be masked.\n",
        "        \"\"\"\n",
        "        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n",
        "        if start_positions is not None:\n",
        "            slen, hsz = hidden_states.shape[-2:]\n",
        "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions) # shape (bsz, 1, hsz)\n",
        "            start_states = start_states.expand(-1, slen, -1) # shape (bsz, slen, hsz)\n",
        "\n",
        "        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n",
        "        x = self.activation(x)\n",
        "        x = self.LayerNorm(x)\n",
        "        x = self.dense_1(x).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerAnswerClass(nn.Module):\n",
        "    \"\"\" Compute SQuAD 2.0 answer class from classification and start tokens hidden states. \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerAnswerClass, self).__init__()\n",
        "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden_states, start_states=None, start_positions=None, cls_index=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            One of ``start_states``, ``start_positions`` should be not None.\n",
        "            If both are set, ``start_positions`` overrides ``start_states``.\n",
        "            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n",
        "                hidden states of the first tokens for the labeled span.\n",
        "            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "                position of the first token for the labeled span.\n",
        "            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n",
        "                position of the CLS token. If None, take the last token.\n",
        "            note(Original repo):\n",
        "                no dependency on end_feature so that we can obtain one single `cls_logits`\n",
        "                for each sample\n",
        "        \"\"\"\n",
        "        hsz = hidden_states.shape[-1]\n",
        "        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n",
        "        if start_positions is not None:\n",
        "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions).squeeze(-2) # shape (bsz, hsz)\n",
        "\n",
        "        if cls_index is not None:\n",
        "            cls_index = cls_index[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, hsz)\n",
        "        else:\n",
        "            cls_token_state = hidden_states[:, -1, :] # shape (bsz, hsz)\n",
        "\n",
        "        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_1(x).squeeze(-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SQuADHead(nn.Module):\n",
        "    r\"\"\" A SQuAD head inspired by XLNet.\n",
        "    Parameters:\n",
        "        config (:class:`~pytorch_transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n",
        "    Inputs:\n",
        "        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``\n",
        "            hidden states of sequence tokens\n",
        "        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            position of the first token for the labeled span.\n",
        "        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            position of the last token for the labeled span.\n",
        "        **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n",
        "            position of the CLS token. If None, take the last token.\n",
        "        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            Whether the question has a possible answer in the paragraph or not.\n",
        "        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n",
        "            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n",
        "            1.0 means token should be masked.\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n",
        "        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n",
        "            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n",
        "        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n",
        "            Indices for the top config.start_n_top start token possibilities (beam-search).\n",
        "        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n",
        "            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
        "        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n",
        "            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
        "        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size,)``\n",
        "            Log probabilities for the ``is_impossible`` label of the answers.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(SQuADHead, self).__init__()\n",
        "        self.start_n_top = config.start_n_top\n",
        "        self.end_n_top = config.end_n_top\n",
        "\n",
        "        self.start_logits = PoolerStartLogits(config)\n",
        "        self.end_logits = PoolerEndLogits(config)\n",
        "        self.answer_class = PoolerAnswerClass(config)\n",
        "\n",
        "    def forward(self, hidden_states, start_positions=None, end_positions=None,\n",
        "                cls_index=None, is_impossible=None, p_mask=None):\n",
        "        outputs = ()\n",
        "\n",
        "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n",
        "            for x in (start_positions, end_positions, cls_index, is_impossible):\n",
        "                if x is not None and x.dim() > 1:\n",
        "                    x.squeeze_(-1)\n",
        "\n",
        "            # during training, compute the end logits based on the ground truth of the start position\n",
        "            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "            if cls_index is not None and is_impossible is not None:\n",
        "                # Predict answerability from the representation of CLS and START\n",
        "                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n",
        "                loss_fct_cls = nn.BCEWithLogitsLoss()\n",
        "                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n",
        "\n",
        "                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n",
        "                total_loss += cls_loss * 0.5\n",
        "\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        else:\n",
        "            # during inference, compute the end logits based on beam search\n",
        "            bsz, slen, hsz = hidden_states.size()\n",
        "            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n",
        "\n",
        "            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n",
        "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n",
        "            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n",
        "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n",
        "\n",
        "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n",
        "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
        "            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n",
        "            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n",
        "\n",
        "            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n",
        "            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n",
        "            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n",
        "\n",
        "            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n",
        "            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n",
        "\n",
        "            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n",
        "\n",
        "        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n",
        "        # or (if labels are provided) (total_loss,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SequenceSummary(nn.Module):\n",
        "    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n",
        "        Args of the config class:\n",
        "            summary_type:\n",
        "                - 'last' => [default] take the last token hidden state (like XLNet)\n",
        "                - 'first' => take the first token hidden state (like Bert)\n",
        "                - 'mean' => take the mean of all tokens hidden states\n",
        "                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)\n",
        "                - 'attn' => Not implemented now, use multi-head attention\n",
        "            summary_use_proj: Add a projection after the vector extraction\n",
        "            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n",
        "            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default\n",
        "            summary_first_dropout: Add a dropout before the projection and activation\n",
        "            summary_last_dropout: Add a dropout after the projection and activation\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(SequenceSummary, self).__init__()\n",
        "\n",
        "        self.summary_type = config.summary_type if hasattr(config, 'summary_use_proj') else 'last'\n",
        "        if self.summary_type == 'attn':\n",
        "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
        "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
        "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.summary = Identity()\n",
        "        if hasattr(config, 'summary_use_proj') and config.summary_use_proj:\n",
        "            if hasattr(config, 'summary_proj_to_labels') and config.summary_proj_to_labels and config.num_labels > 0:\n",
        "                num_classes = config.num_labels\n",
        "            else:\n",
        "                num_classes = config.hidden_size\n",
        "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        self.activation = Identity()\n",
        "        if hasattr(config, 'summary_activation') and config.summary_activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "        self.first_dropout = Identity()\n",
        "        if hasattr(config, 'summary_first_dropout') and config.summary_first_dropout > 0:\n",
        "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
        "\n",
        "        self.last_dropout = Identity()\n",
        "        if hasattr(config, 'summary_last_dropout') and config.summary_last_dropout > 0:\n",
        "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "    def forward(self, hidden_states, cls_index=None):\n",
        "        \"\"\" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n",
        "            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n",
        "                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n",
        "                if summary_type == 'cls_index' and cls_index is None:\n",
        "                    we take the last token of the sequence as classification token\n",
        "        \"\"\"\n",
        "        if self.summary_type == 'last':\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.summary_type == 'first':\n",
        "            output = hidden_states[:, 0]\n",
        "        elif self.summary_type == 'mean':\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.summary_type == 'cls_index':\n",
        "            if cls_index is None:\n",
        "                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n",
        "            else:\n",
        "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
        "                cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))\n",
        "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
        "            output = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, XX, hidden_size)\n",
        "        elif self.summary_type == 'attn':\n",
        "            raise NotImplementedError\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def prune_linear_layer(layer, index, dim=0):\n",
        "    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if layer.bias is not None:\n",
        "        if dim == 1:\n",
        "            b = layer.bias.clone().detach()\n",
        "        else:\n",
        "            b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.requires_grad = False\n",
        "        new_layer.bias.copy_(b.contiguous())\n",
        "        new_layer.bias.requires_grad = True\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "def prune_conv1d_layer(layer, index, dim=1):\n",
        "    \"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n",
        "        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if dim == 0:\n",
        "        b = layer.bias.clone().detach()\n",
        "    else:\n",
        "        b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    new_layer.bias.requires_grad = False\n",
        "    new_layer.bias.copy_(b.contiguous())\n",
        "    new_layer.bias.requires_grad = True\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "def prune_layer(layer, index, dim=None):\n",
        "    \"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n",
        "    elif isinstance(layer, Conv1D):\n",
        "        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n",
        "    else:\n",
        "        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxnOq_WddhFA",
        "outputId": "adf15674-5cb8-4d30-b2c4-85b0e7da7bb9"
      },
      "source": [
        "# MAX_TURN, PREVENT_FACTOR, PROMOTE_FACTOR, PREVENT_LIST, REDUCE_LIST, STOP_LIST\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "MAX_TURN = 6\n",
        "PREVENT_FACTOR = 0.3\n",
        "PROMOTE_FACTOR = 1.1\n",
        "PREVENT_LIST = ['[UNK]', '\"',\"(\",\")\",\"-\",\"[\",\"]\",\"'\",\"&\"]\n",
        "STOP_LIST = set(stopwords.words('french')) | set(['[SEP]', '[PAD]', '[CLS]', 'à', 'de', 'en', 'été', 'est', \"eu\", \"a\" '\"', 'pour', 'sur', 'comme', 'avec', 'par', 'lui', 'près', 'ça', 'quel', '.', ',', '(', ')',\"'\", '%'])\n",
        "REDUCE_LIST = set([\"'\",'s','.',\",\"]) "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azp4DUGfswOj"
      },
      "source": [
        "# WEIGHTS_NAME, CONFIG_NAME, BertForMaskedLM\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\",\n",
        "    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n",
        "}\n",
        "\n",
        "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json\",\n",
        "    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json\",\n",
        "}\n",
        "\n",
        "\n",
        "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\")\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name.split('/')\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n",
        "                l = re.split(r'_(\\d+)', m_name)\n",
        "            else:\n",
        "                l = [m_name]\n",
        "            if l[0] == 'kernel' or l[0] == 'gamma':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'output_bias' or l[0] == 'beta':\n",
        "                pointer = getattr(pointer, 'bias')\n",
        "            elif l[0] == 'output_weights':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'squad':\n",
        "                pointer = getattr(pointer, 'classifier')\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, l[0])\n",
        "                except AttributeError:\n",
        "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "                    continue\n",
        "            if len(l) >= 2:\n",
        "                num = int(l[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == '_embeddings':\n",
        "            pointer = getattr(pointer, 'weight')\n",
        "        elif m_name == 'kernel':\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert pointer.shape == array.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class BertConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "        :class:`~pytorch_transformers.BertConfig` is the configuration class to store the configuration of a\n",
        "        `BertModel`.\n",
        "        Arguments:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "            layer_norm_eps: The epsilon used by LayerNorm.\n",
        "    \"\"\"\n",
        "    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file=30522,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02,\n",
        "                 layer_norm_eps=1e-12,\n",
        "                 **kwargs):\n",
        "        super(BertConfig, self).__init__(**kwargs)\n",
        "        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n",
        "                        and isinstance(vocab_size_or_config_json_file, unicode)):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "            self.layer_norm_eps = layer_norm_eps\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except (ImportError, AttributeError) as e:\n",
        "    logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n",
        "    class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n",
        "        for head in heads:\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "        # Update hyper params\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask, head_mask=None):\n",
        "        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # outputs, (hidden states), (attentions)\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size,\n",
        "                                 config.vocab_size,\n",
        "                                 bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    config_class = BertConfig\n",
        "    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n",
        "    load_tf_weights = load_tf_weights_in_bert\n",
        "    base_model_prefix = \"bert\"\n",
        "\n",
        "    def __init__(self, *inputs, **kwargs):\n",
        "        super(BertPreTrainedModel, self).__init__(*inputs, **kwargs)\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "BERT_START_DOCSTRING = r\"\"\"    The BERT model was proposed in\n",
        "    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n",
        "    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It's a bidirectional transformer\n",
        "    pre-trained using a combination of masked language modeling objective and next sentence prediction\n",
        "    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n",
        "    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n",
        "    refer to the PyTorch documentation for all matter related to general usage and behavior.\n",
        "    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n",
        "        https://arxiv.org/abs/1810.04805\n",
        "    .. _`torch.nn.Module`:\n",
        "        https://pytorch.org/docs/stable/nn.html#module\n",
        "    Parameters:\n",
        "        config (:class:`~pytorch_transformers.BertConfig`): Model configuration class with all the parameters of the model.\n",
        "\"\"\"\n",
        "\n",
        "BERT_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Inputs:\n",
        "        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
        "            (a) For sequence pairs:\n",
        "                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n",
        "                \n",
        "                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n",
        "            (b) For single sequences:\n",
        "                ``tokens:         [CLS] the dog is hairy . [SEP]``\n",
        "                \n",
        "                ``token_type_ids:   0   0   0   0  0     0   0``\n",
        "    \n",
        "            Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n",
        "            See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n",
        "            :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n",
        "        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Indices of positions of each input sequence tokens in the position embeddings.\n",
        "            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n",
        "        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Segment token indices to indicate first and second portions of the inputs.\n",
        "            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n",
        "            corresponds to a `sentence B` token\n",
        "            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n",
        "        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n",
        "            Mask to nullify selected heads of the self-attention modules.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n",
        "\"\"\"\n",
        "\n",
        "@add_start_docstrings(\"The bare Bert Model transformer outputing raw hidden-states without any specific head on top.\",\n",
        "                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n",
        "            Sequence of hidden-states at the output of the last layer of the model.\n",
        "        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n",
        "            Last layer hidden-state of the first token of the sequence (classification token)\n",
        "            further processed by a Linear layer and a Tanh activation function. The Linear\n",
        "            layer weights are trained from the next sentence prediction (classification)\n",
        "            objective during Bert pretraining. This output is usually *not* a good summary\n",
        "            of the semantic content of the input, you're often better with averaging or pooling\n",
        "            the sequence of hidden-states for the whole input sequence.\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids)\n",
        "        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def _resize_token_embeddings(self, new_num_tokens):\n",
        "        old_embeddings = self.embeddings.word_embeddings\n",
        "        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n",
        "        self.embeddings.word_embeddings = new_embeddings\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position_ids=None, head_mask=None):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            if head_mask.dim() == 1:\n",
        "                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n",
        "            elif head_mask.dim() == 2:\n",
        "                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n",
        "            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n",
        "        else:\n",
        "            head_mask = [None] * self.config.num_hidden_layers\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n",
        "        encoder_outputs = self.encoder(embedding_output,\n",
        "                                       extended_attention_mask,\n",
        "                                       head_mask=head_mask)\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with two heads on top as done during the pre-training:\n",
        "    a `masked language modeling` head and a `next sentence prediction (classification)` head. \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForPreTraining(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n",
        "            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n",
        "            in ``[0, ..., config.vocab_size]``\n",
        "        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n",
        "            Indices should be in ``[0, 1]``.\n",
        "            ``0`` indicates sequence B is a continuation of sequence A,\n",
        "            ``1`` indicates sequence B is a random sequence.\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n",
        "        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids)\n",
        "        prediction_scores, seq_relationship_scores = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForPreTraining, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                   self.bert.embeddings.word_embeddings)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n",
        "                next_sentence_label=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top. \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForMaskedLM(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n",
        "            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n",
        "            in ``[0, ..., config.vocab_size]``\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
        "        loss, prediction_scores = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMaskedLM, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                   self.bert.embeddings.word_embeddings)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
        "        if masked_lm_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a `next sentence prediction (classification)` head on top. \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n",
        "            Indices should be in ``[0, 1]``.\n",
        "            ``0`` indicates sequence B is a continuation of sequence A,\n",
        "            ``1`` indicates sequence B is a random sequence.\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Next sequence prediction (classification) loss.\n",
        "        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids)\n",
        "        seq_relationship_scores = outputs[0]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForNextSentencePrediction, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        seq_relationship_score = self.cls(pooled_output)\n",
        "\n",
        "        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            outputs = (next_sentence_loss,) + outputs\n",
        "\n",
        "        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n",
        "    the pooled output) e.g. for GLUE tasks. \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a multiple choice classification head on top (a linear layer on top of\n",
        "    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. \"\"\",\n",
        "    BERT_START_DOCSTRING)\n",
        "class BertForMultipleChoice(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "    Inputs:\n",
        "        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
        "            (a) For sequence pairs:\n",
        "                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n",
        "                \n",
        "                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n",
        "            (b) For single sequences:\n",
        "                ``tokens:         [CLS] the dog is hairy . [SEP]``\n",
        "                \n",
        "                ``token_type_ids:   0   0   0   0  0     0   0``\n",
        "    \n",
        "            Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n",
        "            See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n",
        "            :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n",
        "        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Segment token indices to indicate first and second portions of the inputs.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n",
        "            corresponds to a `sentence B` token\n",
        "            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n",
        "        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Mask to avoid performing attention on padding token indices.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n",
        "            Mask to nullify selected heads of the self-attention modules.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the multiple choice classification loss.\n",
        "            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n",
        "            of the input tensors. (see `input_ids` above)\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss.\n",
        "        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n",
        "            of the input tensors. (see `input_ids` above).\n",
        "            Classification scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n",
        "        choices = [\"Hello, my dog is cute\", \"Hello, my cat is amazing\"]\n",
        "        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n",
        "        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, classification_scores = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultipleChoice, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        num_choices = input_ids.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        outputs = self.bert(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids,\n",
        "                            attention_mask=flat_attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a token classification head on top (a linear layer on top of\n",
        "    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForTokenClassification(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the token classification loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss.\n",
        "        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n",
        "            Classification scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, scores = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForTokenClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n",
        "    the hidden-states output to compute `span start logits` and `span end logits`). \"\"\",\n",
        "    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\n",
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
        "            Position outside of the sequence are not taken into account for computing the loss.\n",
        "        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
        "            Position outside of the sequence are not taken into account for computing the loss.\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
        "        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
        "            Span-start scores (before SoftMax).\n",
        "        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
        "            Span-end scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "    Examples::\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        start_positions = torch.tensor([1])\n",
        "        end_positions = torch.tensor([3])\n",
        "        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss, start_scores, end_scores = outputs[:2]\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
        "                end_positions=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSq7d9ur4Th6"
      },
      "source": [
        "# PreTrainedTokenizer\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n",
        "from __future__ import (absolute_import, division, print_function,\n",
        "                        unicode_literals)\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import six\n",
        "from io import open\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "SPECIAL_TOKENS_MAP_FILE = 'special_tokens_map.json'\n",
        "ADDED_TOKENS_FILE = 'added_tokens.json'\n",
        "\n",
        "class PreTrainedTokenizer(object):\n",
        "    \"\"\" Base class for all tokenizers.\n",
        "    Handle all the shared methods for tokenization and special tokens as well as methods dowloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\n",
        "    This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
        "    Class attributes (overridden by derived classes):\n",
        "        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).\n",
        "        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.\n",
        "        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.\n",
        "    Parameters:\n",
        "        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token``\n",
        "        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token``\n",
        "        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token``\n",
        "        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token``\n",
        "        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token``\n",
        "        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token``\n",
        "        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token``\n",
        "        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won't be split by the tokenization process. Will be associated to ``self.additional_special_tokens``\n",
        "    \"\"\"\n",
        "    vocab_files_names = {}\n",
        "    pretrained_vocab_files_map = {}\n",
        "    max_model_input_sizes = {}\n",
        "\n",
        "    SPECIAL_TOKENS_ATTRIBUTES = [\"bos_token\", \"eos_token\", \"unk_token\", \"sep_token\",\n",
        "                                 \"pad_token\", \"cls_token\", \"mask_token\", \"noi_token\",\n",
        "                                 \"additional_special_tokens\"]\n",
        "\n",
        "    @property\n",
        "    def bos_token(self):\n",
        "        \"\"\" Beginning of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._bos_token is None:\n",
        "            logger.error(\"Using bos_token, but it is not set yet.\")\n",
        "        return self._bos_token\n",
        "\n",
        "    @property\n",
        "    def eos_token(self):\n",
        "        \"\"\" End of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._eos_token is None:\n",
        "            logger.error(\"Using eos_token, but it is not set yet.\")\n",
        "        return self._eos_token\n",
        "\n",
        "    @property\n",
        "    def unk_token(self):\n",
        "        \"\"\" Unknown token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._unk_token is None:\n",
        "            logger.error(\"Using unk_token, but it is not set yet.\")\n",
        "        return self._unk_token\n",
        "\n",
        "    @property\n",
        "    def sep_token(self):\n",
        "        \"\"\" Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\n",
        "        if self._sep_token is None:\n",
        "            logger.error(\"Using sep_token, but it is not set yet.\")\n",
        "        return self._sep_token\n",
        "\n",
        "    @property\n",
        "    def pad_token(self):\n",
        "        \"\"\" Padding token (string). Log an error if used while not having been set. \"\"\"\n",
        "        if self._pad_token is None:\n",
        "            logger.error(\"Using pad_token, but it is not set yet.\")\n",
        "        return self._pad_token\n",
        "\n",
        "    @property\n",
        "    def cls_token(self):\n",
        "        \"\"\" Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"\n",
        "        if self._cls_token is None:\n",
        "            logger.error(\"Using cls_token, but it is not set yet.\")\n",
        "        return self._cls_token\n",
        "\n",
        "    @property\n",
        "    def mask_token(self):\n",
        "        \"\"\" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
        "        if self._mask_token is None:\n",
        "            logger.error(\"Using mask_token, but it is not set yet.\")\n",
        "        return self._mask_token\n",
        "\n",
        "    @property\n",
        "    def noi_token(self):\n",
        "        \"\"\" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
        "        if self._noi_token is None:\n",
        "            logger.error(\"Using noi_token, but it is not set yet.\")\n",
        "        return self._noi_token\n",
        "\n",
        "    @property\n",
        "    def additional_special_tokens(self):\n",
        "        \"\"\" All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. \"\"\"\n",
        "        if self._additional_special_tokens is None:\n",
        "            logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n",
        "        return self._additional_special_tokens\n",
        "\n",
        "    @bos_token.setter\n",
        "    def bos_token(self, value):\n",
        "        self._bos_token = value\n",
        "\n",
        "    @eos_token.setter\n",
        "    def eos_token(self, value):\n",
        "        self._eos_token = value\n",
        "\n",
        "    @unk_token.setter\n",
        "    def unk_token(self, value):\n",
        "        self._unk_token = value\n",
        "\n",
        "    @sep_token.setter\n",
        "    def sep_token(self, value):\n",
        "        self._sep_token = value\n",
        "\n",
        "    @pad_token.setter\n",
        "    def pad_token(self, value):\n",
        "        self._pad_token = value\n",
        "\n",
        "    @cls_token.setter\n",
        "    def cls_token(self, value):\n",
        "        self._cls_token = value\n",
        "\n",
        "    @mask_token.setter\n",
        "    def mask_token(self, value):\n",
        "        self._mask_token = value\n",
        "    \n",
        "    @noi_token.setter\n",
        "    def noi_token(self, value):\n",
        "        self._noi_token = value\n",
        "\n",
        "    @additional_special_tokens.setter\n",
        "    def additional_special_tokens(self, value):\n",
        "        self._additional_special_tokens = value\n",
        "\n",
        "    def __init__(self, max_len=None, **kwargs):\n",
        "        self._bos_token = None\n",
        "        self._eos_token = None\n",
        "        self._unk_token = None\n",
        "        self._sep_token = None\n",
        "        self._pad_token = None\n",
        "        self._cls_token = None\n",
        "        self._mask_token = None\n",
        "        self._noi_token = None\n",
        "        self._additional_special_tokens = []\n",
        "\n",
        "        self.max_len = max_len if max_len is not None else int(1e12)\n",
        "        self.added_tokens_encoder = {}\n",
        "        self.added_tokens_decoder = {}\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
        "                if key == 'additional_special_tokens':\n",
        "                    assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n",
        "                else:\n",
        "                    assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n",
        "                setattr(self, key, value)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *inputs, **kwargs):\n",
        "        r\"\"\" Instantiate a :class:`~pytorch_transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
        "        Parameters:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~pytorch_transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
        "                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
        "            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
        "            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~pytorch_transformers.PreTrainedTokenizer` for details.\n",
        "        Examples::\n",
        "            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
        "            # Download vocabulary from S3 and cache.\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
        "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
        "            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
        "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
        "            # You can link tokens to special vocabulary when instantiating\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
        "            # You should be sure '<unk>' is in the vocabulary when doing that.\n",
        "            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
        "            assert tokenizer.unk_token == '<unk>'\n",
        "        \"\"\"\n",
        "        return cls._from_pretrained(*inputs, **kwargs)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def _from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\n",
        "\n",
        "        s3_models = list(cls.max_model_input_sizes.keys())\n",
        "        vocab_files = {}\n",
        "        if pretrained_model_name_or_path in s3_models:\n",
        "            # Get the vocabulary from AWS S3 bucket\n",
        "            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n",
        "                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            # Get the vocabulary from local files\n",
        "            logger.info(\n",
        "                \"Model name '{}' not found in model shortcut name list ({}). \"\n",
        "                \"Assuming '{}' is a path or url to a directory containing tokenizer files.\".format(\n",
        "                    pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                    pretrained_model_name_or_path))\n",
        "\n",
        "            # Look for the tokenizer main vocabulary files\n",
        "            for file_id, file_name in cls.vocab_files_names.items():\n",
        "                if os.path.isdir(pretrained_model_name_or_path):\n",
        "                    # If a directory is provided we look for the standard filenames\n",
        "                    full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n",
        "                else:\n",
        "                    # If a path to a file is provided we use it (will only work for non-BPE tokenizer using a single vocabulary file)\n",
        "                    full_file_name = pretrained_model_name_or_path\n",
        "                if not os.path.exists(full_file_name):\n",
        "                    logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
        "                    full_file_name = None\n",
        "                vocab_files[file_id] = full_file_name\n",
        "\n",
        "            # Look for the additional tokens files\n",
        "            all_vocab_files_names = {'added_tokens_file': ADDED_TOKENS_FILE,\n",
        "                                     'special_tokens_map_file': SPECIAL_TOKENS_MAP_FILE}\n",
        "\n",
        "            # If a path to a file was provided, get the parent directory\n",
        "            saved_directory = pretrained_model_name_or_path\n",
        "            if os.path.exists(saved_directory) and not os.path.isdir(saved_directory):\n",
        "                saved_directory = os.path.dirname(saved_directory)\n",
        "\n",
        "            for file_id, file_name in all_vocab_files_names.items():\n",
        "                full_file_name = os.path.join(saved_directory, file_name)\n",
        "                if not os.path.exists(full_file_name):\n",
        "                    logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
        "                    full_file_name = None\n",
        "                vocab_files[file_id] = full_file_name\n",
        "\n",
        "            if all(full_file_name is None for full_file_name in vocab_files.values()):\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find tokenizer files\"\n",
        "                    \"at this path or url.\".format(\n",
        "                        pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                        pretrained_model_name_or_path, ))\n",
        "                return None\n",
        "\n",
        "        # Get files from url, cache, or disk depending on the case\n",
        "        try:\n",
        "            resolved_vocab_files = {}\n",
        "            for file_id, file_path in vocab_files.items():\n",
        "                if file_path is None:\n",
        "                    resolved_vocab_files[file_id] = None\n",
        "                else:\n",
        "                    resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            if pretrained_model_name_or_path in s3_models:\n",
        "                logger.error(\"Couldn't reach server to download vocabulary.\")\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find files {} \"\n",
        "                    \"at this path or url.\".format(\n",
        "                        pretrained_model_name_or_path, ', '.join(s3_models),\n",
        "                        pretrained_model_name_or_path, str(vocab_files.keys())))\n",
        "            return None\n",
        "\n",
        "        for file_id, file_path in vocab_files.items():\n",
        "            if file_path == resolved_vocab_files[file_id]:\n",
        "                logger.info(\"loading file {}\".format(file_path))\n",
        "            else:\n",
        "                logger.info(\"loading file {} from cache at {}\".format(\n",
        "                    file_path, resolved_vocab_files[file_id]))\n",
        "\n",
        "        # Set max length if needed\n",
        "        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n",
        "            # if we're using a pretrained model, ensure the tokenizer\n",
        "            # wont index sequences longer than the number of positional embeddings\n",
        "            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n",
        "            if max_len is not None and isinstance(max_len, (int, float)):\n",
        "                kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
        "\n",
        "        # Merge resolved_vocab_files arguments in kwargs.\n",
        "        added_tokens_file = resolved_vocab_files.pop('added_tokens_file', None)\n",
        "        special_tokens_map_file = resolved_vocab_files.pop('special_tokens_map_file', None)\n",
        "        for args_name, file_path in resolved_vocab_files.items():\n",
        "            if args_name not in kwargs:\n",
        "                kwargs[args_name] = file_path\n",
        "        if special_tokens_map_file is not None:\n",
        "            special_tokens_map = json.load(open(special_tokens_map_file, encoding=\"utf-8\"))\n",
        "            for key, value in special_tokens_map.items():\n",
        "                if key not in kwargs:\n",
        "                    kwargs[key] = value\n",
        "\n",
        "        # Instantiate tokenizer.\n",
        "        tokenizer = cls(*inputs, **kwargs)\n",
        "\n",
        "        # Add supplementary tokens.\n",
        "        if added_tokens_file is not None:\n",
        "            added_tok_encoder = json.load(open(added_tokens_file, encoding=\"utf-8\"))\n",
        "            added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n",
        "            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n",
        "            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save the tokenizer vocabulary files (with added tokens) and the\n",
        "            special-tokens-to-class-attributes-mapping to a directory.\n",
        "            This method make sure the full tokenizer can then be re-loaded using the :func:`~pytorch_transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Saving directory ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n",
        "        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n",
        "\n",
        "        with open(special_tokens_map_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n",
        "\n",
        "        with open(added_tokens_file, 'w', encoding='utf-8') as f:\n",
        "            if self.added_tokens_encoder:\n",
        "                out_str = json.dumps(self.added_tokens_encoder, ensure_ascii=False)\n",
        "            else:\n",
        "                out_str = u\"{}\"\n",
        "            f.write(out_str)\n",
        "\n",
        "        vocab_files = self.save_vocabulary(save_directory)\n",
        "\n",
        "        return vocab_files + (special_tokens_map_file, added_tokens_file)\n",
        "\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n",
        "            and special token mappings.\n",
        "            Please use :func:`~pytorch_transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~pytorch_transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def vocab_size(self):\n",
        "        \"\"\" Size of the base vocabulary (without the added tokens) \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Size of the full vocabulary with the added tokens \"\"\"\n",
        "        return self.vocab_size + len(self.added_tokens_encoder)\n",
        "\n",
        "\n",
        "    def add_tokens(self, new_tokens):\n",
        "        \"\"\" Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n",
        "        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n",
        "            Parameters:\n",
        "                new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
        "            Returns:\n",
        "                Number of tokens added to the vocabulary.\n",
        "        Examples::\n",
        "            # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
        "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "            model = BertModel.from_pretrained('bert-base-uncased')\n",
        "            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
        "            print('We have added', num_added_toks, 'tokens')\n",
        "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
        "        \"\"\"\n",
        "        if not new_tokens:\n",
        "            return 0\n",
        "\n",
        "        to_add_tokens = []\n",
        "        for token in new_tokens:\n",
        "            assert isinstance(token, str) or (six.PY2 and isinstance(token, unicode))\n",
        "            if token != self.unk_token and \\\n",
        "                    self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token):\n",
        "                to_add_tokens.append(token)\n",
        "                logger.info(\"Adding %s to the vocabulary\", token)\n",
        "\n",
        "        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n",
        "        added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n",
        "        self.added_tokens_encoder.update(added_tok_encoder)\n",
        "        self.added_tokens_decoder.update(added_tok_decoder)\n",
        "\n",
        "        return len(to_add_tokens)\n",
        "\n",
        "\n",
        "    def add_special_tokens(self, special_tokens_dict):\n",
        "        \"\"\" Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n",
        "            to class attributes. If special tokens are NOT in the vocabulary, they are added\n",
        "            to it (indexed starting from the last index of the current vocabulary).\n",
        "            Parameters:\n",
        "                special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes: [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``].\n",
        "                \n",
        "                    Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
        "            Returns:\n",
        "                Number of tokens added to the vocabulary.\n",
        "        Examples::\n",
        "            # Let's see how to add a new classification token to GPT-2\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            model = GPT2Model.from_pretrained('gpt2')\n",
        "            special_tokens_dict = {'cls_token': '<CLS>'}\n",
        "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "            print('We have added', num_added_toks, 'tokens')\n",
        "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
        "            assert tokenizer.cls_token == '<CLS>'\n",
        "        \"\"\"\n",
        "        if not special_tokens_dict:\n",
        "            return 0\n",
        "\n",
        "        added_tokens = 0\n",
        "        for key, value in special_tokens_dict.items():\n",
        "            assert key in self.SPECIAL_TOKENS_ATTRIBUTES\n",
        "            if key == 'additional_special_tokens':\n",
        "                assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n",
        "                added_tokens += self.add_tokens(value)\n",
        "            else:\n",
        "                assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n",
        "                added_tokens += self.add_tokens([value])\n",
        "            logger.info(\"Assigning %s to the %s key of the tokenizer\", value, key)\n",
        "            setattr(self, key, value)\n",
        "\n",
        "        return added_tokens\n",
        "\n",
        "    def tokenize(self, text, **kwargs):\n",
        "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
        "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
        "            vocabularies (BPE/SentencePieces/WordPieces).\n",
        "            Take care of added tokens.\n",
        "        \"\"\"\n",
        "        def split_on_tokens(tok_list, text):\n",
        "            if not text:\n",
        "                return []\n",
        "            if not tok_list:\n",
        "                return self._tokenize(text, **kwargs)\n",
        "            tok = tok_list[0]\n",
        "            split_text = text.split(tok)\n",
        "            return sum((split_on_tokens(tok_list[1:], sub_text.strip()) + [tok] \\\n",
        "                        for sub_text in split_text), [])[:-1]\n",
        "\n",
        "        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n",
        "        tokenized_text = split_on_tokens(added_tokens, text)\n",
        "        return tokenized_text\n",
        "\n",
        "    def _tokenize(self, text, **kwargs):\n",
        "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
        "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
        "            vocabularies (BPE/SentencePieces/WordPieces).\n",
        "            Do NOT take care of added tokens.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\" Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n",
        "            (resp. a sequence of ids), using the vocabulary.\n",
        "        \"\"\"\n",
        "        if isinstance(tokens, str) or (six.PY2 and isinstance(tokens, unicode)):\n",
        "            return self._convert_token_to_id_with_added_voc(tokens)\n",
        "\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            ids.append(self._convert_token_to_id_with_added_voc(token))\n",
        "        if len(ids) > self.max_len:\n",
        "            logger.warning(\"Token indices sequence length is longer than the specified maximum sequence length \"\n",
        "                           \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
        "                           \"indexing errors\".format(len(ids), self.max_len))\n",
        "        return ids\n",
        "\n",
        "    def _convert_token_to_id_with_added_voc(self, token):\n",
        "        if token in self.added_tokens_encoder:\n",
        "            return self.added_tokens_encoder[token]\n",
        "        return self._convert_token_to_id(token)\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\" Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
        "        \n",
        "        Same doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
        "        \"\"\"\n",
        "        return self.convert_tokens_to_ids(self.tokenize(text))\n",
        "\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n",
        "        \"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n",
        "            (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n",
        "            Args:\n",
        "                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n",
        "        \"\"\"\n",
        "        if isinstance(ids, int):\n",
        "            if ids in self.added_tokens_decoder:\n",
        "                return self.added_tokens_decoder[ids]\n",
        "            else:\n",
        "                return self._convert_id_to_token(ids)\n",
        "        tokens = []\n",
        "        for index in ids:\n",
        "            if index in self.all_special_ids and skip_special_tokens:\n",
        "                continue\n",
        "            if index in self.added_tokens_decoder:\n",
        "                tokens.append(self.added_tokens_decoder[index])\n",
        "            else:\n",
        "                tokens.append(self._convert_id_to_token(index))\n",
        "        return tokens\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string.\n",
        "            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n",
        "            but we often want to remove sub-word tokenization artifacts at the same time.\n",
        "        \"\"\"\n",
        "        return ' '.join(self.convert_ids_to_tokens(tokens))\n",
        "\n",
        "    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n",
        "        \"\"\" Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
        "            with options to remove special tokens and clean up tokenization spaces.\n",
        "        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
        "        \"\"\"\n",
        "        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n",
        "        text = self.convert_tokens_to_string(filtered_tokens)\n",
        "        if clean_up_tokenization_spaces:\n",
        "            text = self.clean_up_tokenization(text)\n",
        "        return text\n",
        "\n",
        "    @property\n",
        "    def special_tokens_map(self):\n",
        "        \"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n",
        "            values ('<unk>', '<cls>'...)\n",
        "        \"\"\"\n",
        "        set_attr = {}\n",
        "        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
        "            attr_value = getattr(self, \"_\" + attr)\n",
        "            if attr_value:\n",
        "                set_attr[attr] = attr_value\n",
        "        return set_attr\n",
        "\n",
        "    @property\n",
        "    def all_special_tokens(self):\n",
        "        \"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n",
        "            (cls_token, unk_token...).\n",
        "        \"\"\"\n",
        "        all_toks = []\n",
        "        set_attr = self.special_tokens_map\n",
        "        for attr_value in set_attr.values():\n",
        "            all_toks = all_toks + (attr_value if isinstance(attr_value, (list, tuple)) else [attr_value])\n",
        "        all_toks = list(set(all_toks))\n",
        "        return all_toks\n",
        "\n",
        "    @property\n",
        "    def all_special_ids(self):\n",
        "        \"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n",
        "            class attributes (cls_token, unk_token...).\n",
        "        \"\"\"\n",
        "        all_toks = self.all_special_tokens\n",
        "        all_ids = list(self.convert_tokens_to_ids(t) for t in all_toks)\n",
        "        return all_ids\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_up_tokenization(out_string):\n",
        "        \"\"\" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
        "        \"\"\"\n",
        "        out_string = out_string.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ','\n",
        "                        ).replace(\" ' \", \"'\").replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" do not\", \" don't\"\n",
        "                        ).replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 're\", \"'re\")\n",
        "        return out_string"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zJeAtQX36-T"
      },
      "source": [
        "# BertTokenizer\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Tokenization classes.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from io import open\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {'vocab_file': 'vocab.txt'}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    'vocab_file':\n",
        "    {\n",
        "        'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\",\n",
        "        'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt\",\n",
        "        'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt\",\n",
        "        'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt\",\n",
        "        'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\",\n",
        "        'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt\",\n",
        "        'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt\",\n",
        "        'bert-base-german-cased': \"https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt\",\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt\",\n",
        "        'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt\",\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    'bert-base-uncased': 512,\n",
        "    'bert-large-uncased': 512,\n",
        "    'bert-base-cased': 512,\n",
        "    'bert-large-cased': 512,\n",
        "    'bert-base-multilingual-uncased': 512,\n",
        "    'bert-base-multilingual-cased': 512,\n",
        "    'bert-base-chinese': 512,\n",
        "    'bert-base-german-cased': 512,\n",
        "    'bert-large-uncased-whole-word-masking': 512,\n",
        "    'bert-large-cased-whole-word-masking': 512,\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
        "    'bert-base-cased-finetuned-mrpc': 512,\n",
        "}\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        tokens = reader.readlines()\n",
        "    for index, token in enumerate(tokens):\n",
        "        token = token.rstrip('\\n')\n",
        "        vocab[token] = index\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class BertTokenizer(PreTrainedTokenizer):\n",
        "    r\"\"\"\n",
        "    Constructs a BertTokenizer.\n",
        "    :class:`~pytorch_transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
        "    Args:\n",
        "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
        "        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n",
        "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
        "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
        "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
        "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
        "            do_wordpiece_only=False\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n",
        "                 unk_token=\"[UNK]\", sep_token=\"[SEP]\", pad_token=\"[PAD]\", cls_token=\"[CLS]\",\n",
        "                 mask_token=\"[MASK]\", tokenize_chinese_chars=True, **kwargs):\n",
        "        \"\"\"Constructs a BertTokenizer.\n",
        "        Args:\n",
        "            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
        "            **do_lower_case**: (`optional`) boolean (default True)\n",
        "                Whether to lower case the input\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **do_basic_tokenize**: (`optional`) boolean (default True)\n",
        "                Whether to do basic tokenization before wordpiece.\n",
        "            **never_split**: (`optional`) list of string\n",
        "                List of tokens which will never be split during tokenization.\n",
        "                Only has an effect when do_basic_tokenize=True\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n",
        "                                            pad_token=pad_token, cls_token=cls_token,\n",
        "                                            mask_token=mask_token, **kwargs)\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict(\n",
        "            [(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.do_basic_tokenize = do_basic_tokenize\n",
        "        if do_basic_tokenize:\n",
        "            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                                  never_split=never_split,\n",
        "                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        if self.do_basic_tokenize:\n",
        "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
        "                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                    split_tokens.append(sub_token)\n",
        "        else:\n",
        "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = ' '.join(tokens).replace(' ##', '').strip()\n",
        "        return out_string\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES['vocab_file'])\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                                   \" Please check that the vocabulary is not corrupted!\".format(vocab_file))\n",
        "                    index = token_index\n",
        "                writer.write(token + u'\\n')\n",
        "                index += 1\n",
        "        return (vocab_file,)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n",
        "        \"\"\" Instantiate a BertTokenizer from pre-trained vocabulary files.\n",
        "        \"\"\"\n",
        "        if pretrained_model_name_or_path in PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES:\n",
        "            if '-cased' in pretrained_model_name_or_path and kwargs.get('do_lower_case', True):\n",
        "                logger.warning(\"The pre-trained model you are loading is a cased model but you have not set \"\n",
        "                               \"`do_lower_case` to False. We are setting `do_lower_case=False` for you but \"\n",
        "                               \"you may want to check this behavior.\")\n",
        "                kwargs['do_lower_case'] = False\n",
        "            elif '-cased' not in pretrained_model_name_or_path and not kwargs.get('do_lower_case', True):\n",
        "                logger.warning(\"The pre-trained model you are loading is an uncased model but you have set \"\n",
        "                               \"`do_lower_case` to False. We are setting `do_lower_case=True` for you \"\n",
        "                               \"but you may want to check this behavior.\")\n",
        "                kwargs['do_lower_case'] = True\n",
        "\n",
        "        return super(BertTokenizer, cls)._from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n",
        "        \"\"\" Constructs a BasicTokenizer.\n",
        "        Args:\n",
        "            **do_lower_case**: Whether to lower case the input.\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
        "                Whether to tokenize Chinese characters.\n",
        "                This should likely be deactivated for Japanese:\n",
        "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
        "        \"\"\"\n",
        "        if never_split is None:\n",
        "            never_split = []\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = never_split\n",
        "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
        "\n",
        "    def tokenize(self, text, never_split=None):\n",
        "        \"\"\" Basic Tokenization of a piece of text.\n",
        "            Split on \"white spaces\" only, for sub-word tokenization, see WordPieceTokenizer.\n",
        "        Args:\n",
        "            **never_split**: (`optional`) list of str\n",
        "                Kept for backward compatibility purposes.\n",
        "                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n",
        "                List of token not to split.\n",
        "        \"\"\"\n",
        "        never_split = self.never_split + (never_split if never_split is not None else [])\n",
        "        text = self._clean_text(text)\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        if self.tokenize_chinese_chars:\n",
        "            text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in never_split:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text, never_split=None):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if never_split is not None and text in never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer`.\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xgv2DIE4pez"
      },
      "source": [
        "# AdamW, WarmupLinearSchedule\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
        "\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ConstantLRSchedule(LambdaLR):\n",
        "    \"\"\" Constant learning rate schedule.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, last_epoch=-1):\n",
        "        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n",
        "\n",
        "\n",
        "class WarmupConstantSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then constant.\n",
        "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
        "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then linear decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
        "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
        "\n",
        "\n",
        "class WarmupCosineWithHardRestartsSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine cycles with hard restarts.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
        "        learning rate (with hard restarts).\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=1., last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        if progress >= 1.0:\n",
        "            return 0.0\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * ((float(self.cycles) * progress) % 1.0))))\n",
        "\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    \"\"\" Implements Adam algorithm with weight decay fix.\n",
        "    Parameters:\n",
        "        lr (float): learning rate. Default 1e-3.\n",
        "        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\n",
        "        eps (float): Adams epsilon. Default: 1e-6\n",
        "        weight_decay (float): Weight decay. Default: 0.0\n",
        "        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1]  < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n",
        "                        correct_bias=correct_bias)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr']\n",
        "                if group['correct_bias']:  # No bias correction for Bert\n",
        "                    bias_correction1 = 1.0 - beta1 ** state['step']\n",
        "                    bias_correction2 = 1.0 - beta2 ** state['step']\n",
        "                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                # Add weight decay at the end (fixed version)\n",
        "                if group['weight_decay'] > 0.0:\n",
        "                    p.data.add_(-group['lr'] * group['weight_decay'], p.data)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwTyXZQj9yRj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4e58zaxjtjl"
      },
      "source": [
        "from argparse import ArgumentParser\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import logging\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, Sampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUeRqDmc95jQ"
      },
      "source": [
        "# Dataset block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGlE4yiJ5ezP"
      },
      "source": [
        "InputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask segment_ids lm_label_ids \")\n",
        "\n",
        "log_format = '%(asctime)-10s: %(message)s'\n",
        "logging.basicConfig(level=logging.INFO, format=log_format)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class Node(object):\n",
        "    def __init__(self, input_ids, segment_ids, input_mask, score, shift, length, pos_start, input_len_start):\n",
        "        super(Node, self).__init__()\n",
        "        self.input_ids = input_ids\n",
        "        self.segment_ids = segment_ids  # parent Node, None for root\n",
        "        self.input_mask = input_mask\n",
        "        self.score = score\n",
        "        self.shift = shift\n",
        "        self.length=length\n",
        "        self.pos_start=pos_start\n",
        "        self.input_len_start=input_len_start\n",
        "seed = 0\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-Tj0_k_5j6X"
      },
      "source": [
        "def convert_example_to_features(example, tokenizer, max_seq_length, tokenizing = False):\n",
        "    tokens = [\"[CLS]\"] + example\n",
        "    \n",
        "    if tokenizing:\n",
        "        input_ids = tokenizer.encode(\" \".join(tokens))\n",
        "    else:\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    input_array = np.zeros(max_seq_length, dtype=np.int)\n",
        "    input_array[:len(input_ids)] = input_ids\n",
        "\n",
        "    mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
        "    mask_array[:len(input_ids)] = 1\n",
        "\n",
        "    segment_array = np.zeros(max_seq_length, dtype=np.bool)\n",
        "\n",
        "    lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-1)\n",
        "\n",
        "    features = InputFeatures(input_ids=input_array,\n",
        "                             input_mask=mask_array,\n",
        "                             segment_ids=segment_array,\n",
        "                             lm_label_ids=lm_label_array,\n",
        "                             )\n",
        "    return features"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52vA1Pq-5vOU"
      },
      "source": [
        "class PregeneratedDataset(Dataset):\n",
        "    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n",
        "        self.vocab = tokenizer.vocab\n",
        "        self.tokenizer = tokenizer\n",
        "        self.epoch = epoch\n",
        "        self.data_epoch = epoch % num_data_epochs\n",
        "        data_file = training_path\n",
        "        num_samples = sum(1 for line in open(data_file))\n",
        "        self.num_samples = num_samples\n",
        "        seq_len = 256\n",
        "        self.temp_dir = None\n",
        "        self.working_dir = None\n",
        "        if reduce_memory:\n",
        "            self.temp_dir = TemporaryDirectory()\n",
        "            self.working_dir = Path(self.temp_dir.name)\n",
        "            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n",
        "                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n",
        "            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n",
        "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
        "            segment_ids = np.memmap(filename=self.working_dir/'segment_ids.memmap',\n",
        "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
        "            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n",
        "                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n",
        "            lm_label_ids[:] = -1\n",
        "        else:\n",
        "            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
        "            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
        "            segment_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
        "            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=-1)\n",
        "            \n",
        "        logging.info(f\"Loading training examples for epoch {epoch}\")\n",
        "        with open(data_file, 'r') as f:\n",
        "            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
        "                if i >= num_samples:\n",
        "                    break\n",
        "                line = line.strip()\n",
        "                example = line.split()\n",
        "                features = convert_example_to_features(example, tokenizer, seq_len)\n",
        "                input_ids[i] = features.input_ids\n",
        "                segment_ids[i] = features.segment_ids\n",
        "                input_masks[i] = features.input_mask\n",
        "                lm_label_ids[i] = features.lm_label_ids\n",
        "        if i != num_samples - 1:\n",
        "            logging.info(\"i={} not equal to num_samples={}\".format(i, num_samples))\n",
        "        logging.info(\"Loading complete!\")\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.input_ids = input_ids\n",
        "        self.input_masks = input_masks\n",
        "        self.segment_ids = segment_ids\n",
        "        self.lm_label_ids = lm_label_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n",
        "                torch.tensor(self.input_masks[item].astype(np.int64)),\n",
        "                torch.tensor(self.segment_ids[item].astype(np.int64)),\n",
        "                torch.tensor(self.lm_label_ids[item].astype(np.int64)),\n",
        "                )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEXoOo-k-QGq"
      },
      "source": [
        "# Decoding block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl4_MeF15znl"
      },
      "source": [
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    \n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    return logits"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyWM9WDt54zc"
      },
      "source": [
        "def greedy_search(model, input_ids, segment_ids, input_mask, device='cuda', temperature=1.0, args=None, tokenizer=None, prevent=None, promote=None, reduce=None, verbose = None):\n",
        "    verbose = 0\n",
        "    if not verbose:\n",
        "        verbose = verbose\n",
        "    zero_list = [\"[\", \"]\", \"(\", \")\"]\n",
        "    zero_ids = [ tokenizer.vocab.get(x) for x in zero_list]\n",
        "    if verbose != 0:\n",
        "        print(\"\\nInput %s\" % (\" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) for x in input_ids[0].detach().cpu().numpy() if x!=0])))\n",
        "    for ip in range(MAX_TURN):\n",
        "        with torch.no_grad():\n",
        "            result= model(input_ids, segment_ids, input_mask)\n",
        "            mask_prediction_scores = result[0]\n",
        "            input_len = torch.sum(input_mask,1)\n",
        "\n",
        "            noi_temp = min(float(ip) / noi_decay, 1.0) \n",
        "            mask_prediction_scores[:,:,1] = mask_prediction_scores[:,:,1] * noi_temp\n",
        "            logits = mask_prediction_scores / temperature\n",
        "\n",
        "            if prevent:\n",
        "                for p in prevent:\n",
        "                    logits[:,:,p] = logits[:,:,p] * PREVENT_FACTOR\n",
        "            if reduce:\n",
        "                reduce_factor = min(float(ip) / reduce_decay, 1.0) \n",
        "                for p in reduce:\n",
        "                    logits[:,:,p] = logits[:,:,p] * reduce_factor\n",
        "            if promote:\n",
        "                for p in promote:\n",
        "                    logits[:,:,p] = logits[:,:,p] * PROMOTE_FACTOR \n",
        "            if lessrepeat:\n",
        "                for p in input_ids.cpu().numpy()[0]:\n",
        "                    logits[:,:,p] = logits[:,:,p] * 0.8            \n",
        "\n",
        "            logits[:,:, zero_ids] = -1e10\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            input_ids_new = torch.zeros_like(input_ids)\n",
        "            top_predicts = torch.zeros([input_ids.shape[0], input_ids.shape[1], 3], dtype=torch.long)\n",
        "\n",
        "            mask_predicts = probs.argmax(2)\n",
        "            for t in range(max_seq_length):\n",
        "                top_predicts[:,t] = torch.topk(probs[:,t,:], k=3)[1]\n",
        "\n",
        "\n",
        "            input_mask_new = torch.zeros_like(input_mask)\n",
        "            logit_new = torch.zeros_like(input_ids,dtype=torch.float)\n",
        "            input_ids_ori = input_ids\n",
        "            top_predicts_new = torch.zeros_like(top_predicts)\n",
        "            i = 0\n",
        "            j = 0\n",
        "            k = 0\n",
        "            sep_tok = tokenizer.vocab['[SEP]']\n",
        "            while np.max([i,j,k]) < max_seq_length-1:\n",
        "                input_ids_new[0,k] = input_ids[0,i]\n",
        "                if input_ids[0,i] == 0: # padding, ignore prediction\n",
        "                    break\n",
        "                if input_ids[0,i] == sep_tok:\n",
        "                    break\n",
        "                i += 1\n",
        "                k += 1\n",
        "\n",
        "                if mask_predicts[0,j].cpu().numpy() != 1:\n",
        "                    input_ids_new[0,k] = mask_predicts[0,j]\n",
        "                    logit_new[0,k] = probs[0,j,mask_predicts[0,j]]\n",
        "                    top_predicts_new[0,k,:] = top_predicts[0,j,:]    \n",
        "                    k+=1\n",
        "                    j+=1\n",
        "                else:\n",
        "                    j+=1\n",
        "            \n",
        "            mask_pos = input_ids_new > 1\n",
        "            input_ids = input_ids_new\n",
        "            input_mask = mask_pos\n",
        "\n",
        "            logit_new = logit_new.detach().cpu().numpy()\n",
        "            top_predicts_new = top_predicts_new.detach().cpu().numpy()\n",
        "            if verbose == 0:\n",
        "                pass\n",
        "            elif verbose == 2:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) + ((\"(\" + \"{:.2f}\".format(float(logit_new[0,i])) + \")\") if logit_new[0,i] > 0 else \"\")  for i, x in enumerate(input_ids[0].detach().cpu().numpy()) if x!=0])))\n",
        "            elif verbose == 3:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) + ((\"(\" + \"{:.2f}\".format(float(logit_new[0,i])) + \" \"+ \" \".join([str(tokenizer.ids_to_tokens.get(y, \"noa\").encode('ascii', 'ignore').decode('ascii')) for y in top_predicts_new[0,i,:]]) + \")\") if logit_new[0,i] > 0 else \"\")  for i, x in enumerate(input_ids[0].detach().cpu().numpy()) if x!=0])))\n",
        "            else:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) for x in input_ids[0].detach().cpu().numpy() if x!=0])))\n",
        "    return input_ids"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNZ62H5R-Y56"
      },
      "source": [
        "# Sample generation function definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2855UX0a8fPT"
      },
      "source": [
        "def sample_generate(model, input_ids, segment_ids, input_mask, device='cuda', temperature=0.9, tokenizer=None, sample_num=1, top_k=10, top_p=0.9, \n",
        "                    prevent=None, promote=None, reduce=None, verbose = None):\n",
        "    verbose = 0\n",
        "    if not verbose:\n",
        "        verbose = verbose\n",
        "    zero_list = [\"[\", \"]\", \"(\", \")\"]\n",
        "    zero_ids = [ tokenizer.vocab.get(x) for x in zero_list]\n",
        "    if verbose != 0:\n",
        "        print(\"\\nInput %s\" % (\" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) for x in input_ids[0].detach().cpu().numpy() if x!=0])))\n",
        "    for ip in range(MAX_TURN):\n",
        "        with torch.no_grad():\n",
        "            result= model(input_ids, segment_ids, input_mask)\n",
        "          \n",
        "            mask_prediction_scores = result[0]\n",
        "           \n",
        "            input_len = torch.sum(input_mask,1)\n",
        "            \n",
        "            noi_temp = min(float(ip) / noi_decay, 1.0) \n",
        "            \n",
        "            mask_prediction_scores[:,:,1] = mask_prediction_scores[:,:,1] * noi_temp\n",
        "           \n",
        "            logits = mask_prediction_scores / temperature\n",
        "                      \n",
        "            if prevent:\n",
        "                for p in prevent:\n",
        "                  logits[:,:,p] = logits[:,:,p] * PREVENT_FACTOR \n",
        "            if reduce:\n",
        "                reduce_factor = min(float(ip) / reduce_decay, 1.0) \n",
        "                for p in reduce:\n",
        "                  if p < model.config.vocab_size:\n",
        "                    logits[:,:,p] = logits[:,:,p] * reduce_factor\n",
        "            if promote:\n",
        "                for p in promote:\n",
        "                    logits[:,:,p] = logits[:,:,p] * PROMOTE_FACTOR \n",
        "            if lessrepeat:\n",
        "                for p in input_ids.cpu().numpy()[0]:\n",
        "                    logits[:,:,p] = logits[:,:,p] * 0.8\n",
        "            \n",
        "            \n",
        "\n",
        "            logits[:,:, zero_ids] = -1e10\n",
        "            for i in range(max_seq_length):\n",
        "                logits[:,i] = top_k_top_p_filtering(logits[:,i].squeeze(), top_k = top_k, top_p = top_p)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            input_ids_new = torch.zeros_like(input_ids)\n",
        "            top_predicts = torch.zeros([input_ids.shape[0], input_ids.shape[1], 3], dtype=torch.long)\n",
        "            mask_predicts = torch.zeros_like(input_ids, dtype=torch.long)\n",
        "            for t in range(max_seq_length):\n",
        "                mask_predicts[:,t] =torch.multinomial(probs[:,t,:], num_samples=1)\n",
        "                top_predicts[:,t] = torch.topk(probs[:,t,:], k=3)[1]\n",
        "\n",
        "\n",
        "            logit_new = torch.zeros_like(input_ids,dtype=torch.float)\n",
        "            input_ids_ori = input_ids\n",
        "            top_predicts_new = torch.zeros_like(top_predicts)\n",
        "            i = 0\n",
        "            j = 0\n",
        "            k = 0\n",
        "            sep_tok = tokenizer.vocab['[SEP]']\n",
        "            while np.max([i,j,k]) < max_seq_length-1:                \n",
        "                input_ids_new[0,k] = input_ids[0,i]\n",
        "                if input_ids[0,i] == 0: # padding, ignore prediction\n",
        "                    break\n",
        "                if input_ids[0,i] == sep_tok:\n",
        "                    break\n",
        "                i += 1\n",
        "                k += 1\n",
        "\n",
        "                if mask_predicts[0,j].cpu().numpy() != 1:\n",
        "                    input_ids_new[0,k] = mask_predicts[0,j]\n",
        "                    logit_new[0,k] = probs[0,j,mask_predicts[0,j]]\n",
        "                    top_predicts_new[0,k,:] = top_predicts[0,j,:]                    \n",
        "                    k+=1\n",
        "                    j+=1\n",
        "                else:\n",
        "                    j+=1\n",
        "            \n",
        "            mask_pos = input_ids_new > 1\n",
        "            input_ids = input_ids_new\n",
        "            input_mask = mask_pos\n",
        "            \n",
        "\n",
        "            logit_new = logit_new.detach().cpu().numpy()\n",
        "            top_predicts_new = top_predicts_new.detach().cpu().numpy()\n",
        "            if verbose == 0:\n",
        "                pass\n",
        "            elif verbose == 2:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) + ((\"(\" + \"{:.2f}\".format(float(logit_new[0,i])) + \")\") if logit_new[0,i] > 0 else \"\")  for i, x in enumerate(input_ids[0].detach().cpu().numpy()) if x!=0])))\n",
        "            elif verbose == 3:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) + ((\"(\" + \"{:.2f}\".format(float(logit_new[0,i])) + \" \"+ \" \".join([str(tokenizer.ids_to_tokens.get(y, \"noa\").encode('ascii', 'ignore').decode('ascii')) for y in top_predicts_new[0,i,:]]) + \")\") if logit_new[0,i] > 0 else \"\")  for i, x in enumerate(input_ids[0].detach().cpu().numpy()) if x!=0])))\n",
        "            else:\n",
        "                print(\"Round %d: %s\" % (ip, \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) for x in input_ids[0].detach().cpu().numpy() if x!=0])))\n",
        "    return input_ids"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l30nwC4O-h8H"
      },
      "source": [
        "# Model upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfu24ozj-tVx"
      },
      "source": [
        "!wget \"https://orangepointer.blob.core.windows.net/files/finetunned_model.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umyA6MeWBl5_"
      },
      "source": [
        "!wget \"https://orangepointer.blob.core.windows.net/files/keywords.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF7WjHmdisu3"
      },
      "source": [
        "bert_model = \"./model\"\n",
        "model = BertForMaskedLM.from_pretrained(bert_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lxZU2q_Axj"
      },
      "source": [
        "# Generation setup & run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc6IXkXz6KiT"
      },
      "source": [
        "noi_decay = 1\n",
        "reduce_decay = 1\n",
        "prevent = True\n",
        "reduce_stop = True\n",
        "lessrepeat = True\n",
        "max_seq_length = 256\n",
        "\n",
        "device = \"cuda\"\n",
        "local_rank = -1\n",
        "no_cuda = False\n",
        "output_dir = None\n",
        "fp16 = False\n",
        "\n",
        "do_lower_case = False\n",
        "\n",
        "#bert_model = \"./model\"\n",
        "keyfile = \"keywords.txt\"\n",
        "\n",
        "batch_size = 1 # Should not be more than 1 (original code constraint)\n",
        "output_path = \"output_data.txt\"\n",
        "\n",
        "#sampling_type = \"greedy\"\n",
        "sampling_type = \"sampling\""
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MUIH8lZdrLJ"
      },
      "source": [
        "if not output_dir:\n",
        "  output_dir = bert_model\n",
        "\n",
        "  epoch_file = keyfile\n",
        "  \n",
        "  # Setup CUDA, GPU & distributed training\n",
        "  if local_rank == -1 or no_cuda:\n",
        "      device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
        "      n_gpu = torch.cuda.device_count()\n",
        "  else: # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "      torch.cuda.set_device(local_rank)\n",
        "      device = torch.device(\"cuda\", local_rank)\n",
        "      n_gpu = 1\n",
        "      \n",
        "      # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "      torch.distributed.init_process_group(backend='nccl')\n",
        "\n",
        "  device = device\n",
        "  \n",
        "  # Setup logging\n",
        "  logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                      datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                      level = logging.INFO if local_rank in [-1, 0] else logging.WARN)\n",
        "  logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                  local_rank, device, n_gpu, bool(local_rank != -1), fp16)\n",
        "  # Set seed\n",
        "  set_seed(seed)\n",
        "\n",
        "  output_mode = \"classification\"\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
        "    \n",
        "  # Prepare model\n",
        "  model = BertForMaskedLM.from_pretrained(bert_model)\n",
        "\n",
        "  sep_tok = tokenizer.vocab['[SEP]']\n",
        "  cls_tok = tokenizer.vocab['[CLS]']\n",
        "  pad_tok = tokenizer.vocab['[PAD]']\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  epoch_dataset = PregeneratedDataset(epoch=0, training_path=keyfile, tokenizer=tokenizer, num_data_epochs=1)\n",
        "  epoch_sampler = SequentialSampler(epoch_dataset)\n",
        "  generate_dataloader = DataLoader(epoch_dataset, sampler=epoch_sampler,batch_size=batch_size)\n",
        "  file_name = os.path.join(output_dir, f\"{sampling_type}.txt\")\n",
        "  f = open(file_name, \"w\", 1)\n",
        "\n",
        "\n",
        "  logging.info(\"***** Running generation *****\")\n",
        "  logging.info(f\"  Num examples = {epoch_dataset.num_samples}\")\n",
        "  logging.info(\"  Batch size = %d\", batch_size)\n",
        "  logging.info(f\"  Save to {file_name}\")\n",
        "\n",
        "\n",
        "  prevent = [ tokenizer.vocab.get(x) for x in PREVENT_LIST] if prevent else None\n",
        "  \n",
        "  if reduce_stop:\n",
        "    reduce_l = REDUCE_LIST |  STOP_LIST\n",
        "  reduce = None\n",
        "  if prevent:\n",
        "      reduce = [ tokenizer.vocab.get(x) for x in reduce_l]\n",
        "      reduce = [s for s in reduce if s]\n",
        "\n",
        "\n",
        "  with tqdm(total=len(generate_dataloader), desc=f\"Epoch {0}\") as pbar:\n",
        "      for step, batch in enumerate(generate_dataloader):\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          input_ids, input_mask, segment_ids, lm_label_ids = batch\n",
        "          if sampling_type == \"greedy\":\n",
        "              predict_ids = greedy_search(model, input_ids, segment_ids, input_mask, tokenizer=tokenizer, prevent=prevent, reduce=reduce)\n",
        "          elif sampling_type == 'sampling':\n",
        "              predict_ids = sample_generate(model, input_ids, segment_ids, input_mask, temperature=0.9, tokenizer=tokenizer, prevent=prevent, reduce=reduce)\n",
        "          else:\n",
        "              raise NotImplementedError\n",
        "          output =  \" \".join([str(tokenizer.ids_to_tokens.get(x, \"noa\").encode('ascii', 'ignore').decode('ascii')) for x in predict_ids[0].detach().cpu().numpy() if x!=sep_tok and x != pad_tok and x != cls_tok]) + \"\\n\" \n",
        "          print(output)\n",
        "          output = output.replace(\" ##\", \"\")\n",
        "          f.write(output)\n",
        "          pbar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}